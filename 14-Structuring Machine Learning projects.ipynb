{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structuring Machine Learning projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [1. Machine Learning Strategy](#chapter1)\n",
    "    * [1.1 Introduction](#section_1_1)\n",
    "    * [1.2 Setting up your goal](#section_1_2)\n",
    "* [2. ](#chapter2)\n",
    "    * [2.1 ](#section_2_1)\n",
    "    * [2.2 ](#section_2_2)\n",
    "* [3. ](#chapter3)\n",
    "    * [3.1 ](#section_3_1)\n",
    "    * [3.2 ](#section_3_2)\n",
    "* [4. ](#chapter4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Machine Learning Strategy <a class=\"anchor\" id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Introduction <a class=\"anchor\" id=\"section_1_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to structure a Machine Learning project?**\n",
    "\n",
    "What is machine Learning strategy?\n",
    "\n",
    "Let's say we are working on an application to recognize cat. After working it for some time, we have gotten an 90% accuracy, but now we want to improve our model.\n",
    "\n",
    "We have several ideas to improve our system like:\n",
    "- collecting more data\n",
    "- collecting more diverse training set\n",
    "- training algorithm longer with gradient descent\n",
    "- Trying Dropout\n",
    "- Trying Adam\n",
    "...\n",
    "\n",
    "\n",
    "When we try to improve deep learning system we have often lot of ideas we can try. The problem is that if we choose poorly, it is possible that we spend a lot of time testing an poor idea without good results at the end.\n",
    "\n",
    "\n",
    "We need to find a number of strategies, that is ways of analyzing machine learning problem that will point us in the direction of the most promising things to try.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/10-ML strategy/introduction.PNG\" width =\"600px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Orthogonalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>ML</th>\n",
    "            <th>knob to tune</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>Fit Training set well on cost function</td>\n",
    "            <td>Bigger Network, Adam ...</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Fit dev set well on cost function</td>\n",
    "            <td>Regularization, Bigger Training set ...</td>\n",
    "        </tr>\n",
    "         <tr>\n",
    "            <td>Fit Test set well on cost function</td>\n",
    "            <td>Bigger dev set</td>\n",
    "        </tr>\n",
    "         <tr>\n",
    "            <td>Perform well in real world</td>\n",
    "            <td>Change dev set or cost function</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, it's nice if you can look at your system and say, this piece of it is wrong. It does not do well on the training set, it does not do well on the dev set, it does not do well on the test set, or it's doing well on the test set but just not in the real world. But figure out exactly what's wrong, and then have exactly one knob, or a specific set of knobs that helps to just solve that problem that is limiting the performance of machine learning system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Setting up your goal <a class=\"anchor\" id=\"section_1_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u>Single Number Evaluation Metric</u>**\n",
    "\n",
    "When teams starting on a machine learning project, It is often recommended to set up a single real number evaluation metric for the problem.\n",
    "\n",
    "\n",
    "<u>Example :</u>\n",
    "<center><img src=\"images/10-ML strategy/example-metric.PNG\" width =\"300px\"></center>\n",
    "\n",
    "In this example we have two classifier of cat, A and B. One way to evaluate the performance of the classifiers is to look at its precision and recall.\n",
    "\n",
    "According to this two metrics, classifier A is better than B at Recall, but B is better than A at Precision. Then we are not sure which classifier is better.\n",
    "\n",
    "If we are trying a lot of ideas, a lot of different hyperparameters, we want to quickly try out several classifiers and pick out the best ones. But with two evaluations metrics it is difficult to know how to quickly pick one. \n",
    "\n",
    "So in this example, rather than using recall and precision, we should take a metrics that combines this two. In machine learning a way to combine precision and recall is F1 score.\n",
    "\n",
    "<center><img src=\"images/10-ML strategy/example-metric2.PNG\" width =\"300px\"></center>\n",
    "\n",
    "One Evaluation metric allows us to quickly tell if classifier A or classifier B is better. And so it speeds up the iterative process improving our machine learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u>Train/Dev/Test distributions</u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
