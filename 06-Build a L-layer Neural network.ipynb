{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a L-layer Neural Network.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Recap](#chapter0)\n",
    "* [1. L-laers Neural Network Model](#chapter1)\n",
    "    * [1.1 Functions of our L-layers Neural Network ](#section_1_1)\n",
    "        * [1.1.1 Initialize parameters](#section_1_1_1)\n",
    "        * [1.1.2 Forward propagation](#section_1_1_2)\n",
    "        * [1.1.3 Cost function](#section_1_1_3)\n",
    "        * [1.1.4 Backward Propagation](#section_1_1_4)\n",
    "        * [1.1.5 Update parameters](#section_1_1_5) \n",
    "    * [1.2 L-layer Model](#section_1_2)\n",
    "* [2. Load the Dataset ](#chapter2)\n",
    "    * [2.1 Load the Dataset](#section_2_1)\n",
    "    * [2.2 Display the Data](#section_2_2)\n",
    "    * [2.3 Flatten the data](#section_2_3)\n",
    "    * [2.4 Normalize the data](#section_2_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap  <a class=\"anchor\" id=\"chapter0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Forward Propagation :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/05-Deep Neural network/forward-prop.png\" width = \"600px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{cases}\n",
    "    Z^{[l]} = W^{[l]} X + b^{[l]} \\\\\n",
    "    A^{[l]} = g^{[l]}(Z^{[l]}) \n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Backward Propagation :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/05-Deep Neural network/backward-prop.png\" width = \"600px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{cases}\n",
    "    dZ^{[l]} =  dA^{[l]}  * g^{[l]'}(Z^{[l]}) \\\\\n",
    "    dW^{[l]} = \\frac{1}{m} dZ^{[l]}A^{[l-1]T} \\\\\n",
    "    db^{[l]} = \\frac{1}{m} \\sum dZ^{[l]}    \\\\\n",
    "    dA^{[l-1]} =W^{[l]T}dZ^{[l]} \\\\\n",
    "    dZ^{[l-1]} = W^{[l]T}dZ^{[l]} * g^{[l-1]'}(Z^{[l-1]}) \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "    dZ^{[1]} = W^{[2]T}dZ^{[2]} * g^{[1]'}(Z^{[1]}) \\\\\n",
    "    dW^{[1]} = \\frac{1}{m} dZ^{[1]} X^T \\\\\n",
    "    db^{[1]} = \\frac{1}{m} \\sum  dZ^{[1]}\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Dimension :\n",
    "- m : number of examples\n",
    "\n",
    "$$\\begin{cases}\n",
    "    Z^{[l]},A^{[l]} : (n^{[l]},m) \\\\\n",
    "    W^{[l]}: (n^{[l]},n^{[l-1]})   \\\\\n",
    "    b^{[l]}: (n^{[l]},1)  \\\\\n",
    "    dZ^{[l]},dA^{[l]} : (n^{[l]},m) \\\\\n",
    "    dW^{[l]} : (n^{[l]},n^{[l-1]})   \\\\\n",
    "    db^{[l]}: (n^{[l]},1)\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. L-layers Neural Network model <a class=\"anchor\" id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Functions of our L-layers Neural Network  <a class=\"anchor\" id=\"section_1_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Initialize parameters  <a class=\"anchor\" id=\"section_1_1_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(hidden_layers_dim,n_input,n_output):\n",
    "    \"\"\"\n",
    "    Initialize the l parameters of the L-layer neural network\n",
    "    \n",
    "    Arguments:\n",
    "    hidden_layers_dim -- list of hidden units in the hidden layers\n",
    "    n_input -- features of the input matrix X\n",
    "    n_output -- number units in the output layer \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary containing W1, W2, b1, and b2, ....\n",
    "    \"\"\"\n",
    "\n",
    "    # seed\n",
    "    np.random.seed(3)\n",
    "\n",
    "    # init cache\n",
    "    parameters = {}\n",
    "    l = len(hidden_layers_dim)\n",
    "\n",
    "    for i,n_dim in enumerate(hidden_layers_dim):\n",
    "        \n",
    "        if i == 0:\n",
    "            W = np.random.randn(n_dim,n_input) * 0.01\n",
    "            b = np.zeros((n_dim,1))\n",
    "        else:\n",
    "            W = np.random.randn(n_dim,hidden_layers_dim[i-1]) * 0.01\n",
    "            b = np.zeros((n_dim,1))\n",
    "\n",
    "        # getting params\n",
    "        parameters[f\"W{i+1}\"] = W\n",
    "        parameters[f\"b{i+1}\" ] = b\n",
    "\n",
    "    # output layer\n",
    "    W = np.random.randn(n_output,hidden_layers_dim[-1]) * 0.01\n",
    "    b = np.zeros((n_output,1))\n",
    "    \n",
    "\n",
    "    # getting params\n",
    "    parameters[f\"W{l+1}\"] = W\n",
    "    parameters[f\"b{l+1}\" ] = b\n",
    "\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.01788628,  0.0043651 ,  0.00096497, -0.01863493],\n",
       "        [-0.00277388, -0.00354759, -0.00082741, -0.00627001],\n",
       "        [-0.00043818, -0.00477218, -0.01313865,  0.00884622],\n",
       "        [ 0.00881318,  0.01709573,  0.00050034, -0.00404677],\n",
       "        [-0.0054536 , -0.01546477,  0.00982367, -0.01101068]]),\n",
       " 'b1': array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'W2': array([[-0.01185047, -0.0020565 ,  0.01486148,  0.00236716, -0.01023785],\n",
       "        [-0.00712993,  0.00625245, -0.00160513, -0.00768836, -0.00230031],\n",
       "        [ 0.00745056,  0.01976111, -0.01244123, -0.00626417, -0.00803766],\n",
       "        [-0.02419083, -0.00923792, -0.01023876,  0.01123978, -0.00131914],\n",
       "        [-0.01623285,  0.00646675, -0.00356271, -0.01743141, -0.0059665 ]]),\n",
       " 'b2': array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'W3': array([[-0.00588594, -0.00873882,  0.00029714, -0.02248258, -0.00267762],\n",
       "        [ 0.01013183,  0.00852798,  0.01108187,  0.01119391,  0.01487543],\n",
       "        [-0.01118301,  0.00845833, -0.0186089 , -0.00602885, -0.01914472]]),\n",
       " 'b3': array([[0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'W4': array([[ 0.01048148,  0.01333738, -0.00197415]]),\n",
       " 'b4': array([[0.]])}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "layers_dim = [5,5,3]\n",
    "\n",
    "params = initialize_parameters(layers_dim,4,1)\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Forward propagation  <a class=\"anchor\" id=\"section_1_1_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_function(Z,activation_name):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute the activation function\n",
    "    \n",
    "    Arguments:\n",
    "    activation_name -- name of the activation function choosen\n",
    "    Z -- items\n",
    "\n",
    "    Returns:\n",
    "    activation -- activation value\n",
    "    \"\"\"\n",
    "\n",
    "    if activation_name.lower() == \"sigmoid\":\n",
    "\n",
    "        activation = 1 / (1+np.exp(-Z))\n",
    "\n",
    "    elif activation_name.lower() == \"relu\":\n",
    "\n",
    "        activation = np.maximum(0,Z)\n",
    "\n",
    "    elif activation_name.lower() == \"tanh\":\n",
    "\n",
    "        activation = np.tanh(Z)\n",
    "    else:\n",
    "        # default activation function\n",
    "        activation = np.maximum(0,Z)\n",
    "\n",
    "\n",
    "    assert(activation.shape == Z.shape)\n",
    "\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 100)\n"
     ]
    }
   ],
   "source": [
    "# test activation\n",
    "Z = np.random.randn(10,100)\n",
    "A = activation_function(Z,\"sigmoid\")\n",
    "\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Forward_propagation(X,parameters):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute the forward propagation on the L layers\n",
    "    \n",
    "    Arguments:\n",
    "    X -- Input\n",
    "    parameters -- dictionnary containing the parameters of each layers\n",
    "\n",
    "    Returns:\n",
    "    caches -- list of dictionnaries. each dictionnay contains the linear result and activation of each layer\n",
    "    \"\"\"\n",
    "\n",
    "    # caches and layers\n",
    "    caches = []\n",
    "    L = len(parameters)//2\n",
    "    \n",
    "    # Input X\n",
    "    A_prev = X\n",
    "\n",
    "    for i in range(L):\n",
    "\n",
    "        # getting the parameters of the i-th layers\n",
    "        W = parameters[f\"W{i+1}\"]\n",
    "        b = parameters[f\"b{i+1}\"]\n",
    "\n",
    "        # linear and activation result\n",
    "        Z = np.dot(W,A_prev) + b\n",
    "\n",
    "        # activation\n",
    "        if i == L - 1 :\n",
    "            A = activation_function(Z,\"sigmoid\")\n",
    "        else:\n",
    "            A = activation_function(Z,\"relu\")\n",
    "\n",
    "        # append cache\n",
    "        cache = {\"W\":W,\"b\":b,\"Z\":Z,\"A\":A,\"A_prev\":A_prev}\n",
    "        caches.append(cache)\n",
    "\n",
    "        # change A_prev\n",
    "        A_prev = A\n",
    "\n",
    "    return A, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2)\n",
      "(5, 5)\n",
      "(3, 5)\n",
      "(1, 3)\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "layers_dim = [5,5,3]\n",
    "X = np.random.randn(2,100)\n",
    "params = initialize_parameters(layers_dim,X.shape[0],1)\n",
    "\n",
    "AL,caches = Forward_propagation(X,params)\n",
    "\n",
    "for val in caches:\n",
    "    print(val[\"W\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Cost function  <a class=\"anchor\" id=\"section_1_1_3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(AL,y):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute the cost after the forward propagation\n",
    "\n",
    "    Arguments:\n",
    "    AL -- L-activation \n",
    "    y -- true labels of the dataset dim = (n_y,m) | m examples, n_y nodes of the output layer\n",
    "\n",
    "    Returns:\n",
    "    cost -- cost value\n",
    "    \"\"\"\n",
    "    # m  examples\n",
    "    m = y.shape[1]\n",
    "\n",
    "    # cost\n",
    "    cost = -(1/m) *(np.dot(y,np.log(AL).T) + np.dot((1-y),np.log(1-AL).T))\n",
    "    cost = np.squeeze(cost)\n",
    "\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0045892596933899\n",
      "1.0045892596933899\n"
     ]
    }
   ],
   "source": [
    "# Test the cost\n",
    "y_true = np.random.randint(0,2,(1,100))\n",
    "y_pred = np.random.random((1,100))\n",
    "\n",
    "# check with the true log_loss\n",
    "cost = cost_function(y_pred,y_true)\n",
    "l_cost = log_loss(y_true.T,y_pred.T)\n",
    "print(cost)\n",
    "print(l_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 Backward Propagation  <a class=\"anchor\" id=\"section_1_1_4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_activation_function(dA,Z, function_name):\n",
    "\n",
    "    if function_name.lower() == \"sigmoid\":\n",
    "\n",
    "        Z = Z\n",
    "        s = 1/(1+np.exp(-Z))\n",
    "        \n",
    "        dZ = dA * s * (1-s)\n",
    "\n",
    "    elif function_name.lower() == \"relu\":\n",
    "\n",
    "        Z = Z\n",
    "        dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "       \n",
    "        # When z <= 0, you should set dz to 0 as well. \n",
    "        dZ[Z <= 0] = 0\n",
    "\n",
    "    elif function_name.lower() == \"tanh\":\n",
    "        Z = Z\n",
    "        s = np.tanh(Z)\n",
    "\n",
    "        dZ = dA * (1-np.power(s,2))\n",
    "    \n",
    "    else:\n",
    "        #default: relu\n",
    "        Z = Z\n",
    "        dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "        # When z <= 0, you should set dz to 0 as well. \n",
    "        dZ[Z <= 0] = 0\n",
    "\n",
    "    \n",
    "    return dZ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(AL,y,caches):\n",
    "    \n",
    "    # gradients\n",
    "    gradients = {}\n",
    "\n",
    "    # L layers\n",
    "    L = len(caches)\n",
    "\n",
    "    # m examples\n",
    "    m = y.shape[1]\n",
    "\n",
    "    # dAL \n",
    "    dAL = - (np.divide(y, AL) - np.divide(1 - y, 1 - AL))\n",
    "    \n",
    "    # getting caches variables\n",
    "    current_cache = caches[-1]\n",
    "    WL = current_cache[\"W\"]\n",
    "    ZL = current_cache[\"Z\"]\n",
    "    A_prev = current_cache[\"A_prev\"]\n",
    "    \n",
    "    dZ = backward_activation_function(dAL,ZL,\"sigmoid\")\n",
    "    \n",
    "    dW_temp = (1/m) * np.dot(dZ,A_prev.T)\n",
    "    db_temp = (1/m) * np.sum(dZ,axis=1, keepdims=True)\n",
    "    dA_prev_temp = np.dot(WL.T,dZ)\n",
    "    \n",
    "    # compute the gradient\n",
    "    gradients[\"dW\" + str(L)] = dW_temp\n",
    "    gradients[\"db\" + str(L)] = db_temp\n",
    "    \n",
    "\n",
    "    for i in reversed(range(L-1)):\n",
    "        # getting caches variables\n",
    "        current_cache = caches[i]\n",
    "       \n",
    "        W = current_cache[\"W\"]\n",
    "        Z = current_cache[\"Z\"]\n",
    "        A_prev = current_cache[\"A_prev\"]\n",
    "\n",
    "        dZ = backward_activation_function(dA_prev_temp,Z,\"relu\")\n",
    "       \n",
    "        dW_temp = (1/m) * np.dot(dZ,A_prev.T)\n",
    "        db_temp = (1/m) * np.sum(dZ,axis=1, keepdims=True)\n",
    "        dA_prev_temp = np.dot(W.T,dZ)\n",
    "        \n",
    "        # compute the gradient\n",
    "        gradients[\"dW\" + str(i+1)] = dW_temp\n",
    "        gradients[\"db\" + str(i+1)] = db_temp\n",
    "\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 (5, 10)\n",
      "dW2 (5, 5)\n",
      "dW3 (4, 5)\n",
      "dW4 (1, 4)\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "layers_dim = [5,5,4]\n",
    "X = np.random.randn(10,100)\n",
    "y_true = np.random.randint(0,2,(1,100))\n",
    "params = initialize_parameters(layers_dim,X.shape[0],1)\n",
    "\n",
    "AL,caches = Forward_propagation(X,params)\n",
    "    \n",
    "gradients = backward_propagation(AL,y_true,caches)\n",
    "\n",
    "for i in range(len(caches)):\n",
    "    print(f\"dW{i+1}\",gradients[f\"dW{i+1}\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.5 Update parameters  <a class=\"anchor\" id=\"section_1_1_5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 L-layer Model <a class=\"anchor\" id=\"section_1_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d46af94c2bbce495f1e668725902fa517c90b1782bcfe2fce0dd9868df553d3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
