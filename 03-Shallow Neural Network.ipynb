{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shallow Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "* [1. Neural Network Representation](#chapter1)\n",
    "    * [1.1 Representation](#section_1_1)\n",
    "    * [1.2  Computing a Neural Network output's](#section_1_2)\n",
    "    * [1.3 Vectorizing across multiple examples](#section_1_3)\n",
    "* [2. Activation functions](#chapter2)\n",
    "    * [2.1 Overview of the activation functions](#section_2_1)\n",
    "    * [2.2 Why do you need non-linear activation function](#section_2_2)\n",
    "    * [2.3 Derivatives of activation functions](#section_2_2)\n",
    "* [3. Gradient Descent for one hidden layer Neural network](#chapter3)\n",
    "* [4. Random initialization](#chapter4)\n",
    "* [5. Recap](#chapter5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Neural Network Representation <a class=\"anchor\" id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Representation <a class=\"anchor\" id=\"section_1_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03-shallow neural network/NN-representation.PNG\" width = \"400px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have the input features, x1, x2, x3 stacked up vertically. And this is called the <b>input layer</b> of the neural network.\n",
    "- Then there's another layer of circles. And this is called a <b>hidden layer</b> of the neural network. \n",
    "- The final layer here is formed by, in this case, just one node. And this single-node layer is called the <b>output layer</b>, and is responsible for generating the predicted value\n",
    "\n",
    "> This neural network has <b>Two layers</b>. Indeed we don't count the input layer in a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Computing a Neural Network output's <a class=\"anchor\" id=\"section_1_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A node in a layer does two steps of computation:\n",
    "\n",
    "- 1: It computes z = w.T x + b\n",
    "- 2: It computes the activation function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03-shallow neural network/single-node.PNG\" width = \"300px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So each node in neural network and so in the hidden layer will compute these two steps.\n",
    "\n",
    "By convention the notation are the following:\n",
    "\n",
    "$$ a^{[l]}_i $$\n",
    "- a represents the activation of the layer (input layer a[0] | hidden layer a[1] | output layer a[2])\n",
    "- l means the layer l\n",
    "- i means the node i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Lets compute the activation of the first node in the hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03-shallow neural network/hidden-layer-node1.PNG\" width = \"300px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The representation of X and W are:\n",
    "$$X =\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3  \\end{bmatrix} $$\n",
    "$$W^{[1]}_1 =\\begin{bmatrix} w_{11} \\\\ w_{12} \\\\ w_{13}  \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two steps to compute in this node:\n",
    "\n",
    "$$ Z^{[1]}_1 = W^{[1]T}_1 X + b^{[1]}_1$$\n",
    "$$ a^{[1]}_1 = \\sigma(Z^{[1]}_1) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We repeat this method on each node on the hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03-shallow neural network/hidden-layer-node2.PNG\" width = \"300px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "    \\begin{cases}\n",
    "    Z^{[1]}_1 = W^{[1]T}_1 X + b^{[1]}_1 \\\\\n",
    "    Z^{[1]}_2 = W^{[1]T}_2 X + b^{[1]}_2 \\\\\n",
    "    Z^{[1]}_3 = W^{[1]T}_3 X + b^{[1]}_3 \\\\\n",
    "    Z^{[1]}_4 = W^{[1]T}_4 X + b^{[1]}_4 \n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "$$ \n",
    "    \\begin{cases}\n",
    "    a^{[1]}_1 = \\sigma(Z^{[1]}_1) \\\\\n",
    "    a^{[1]}_2 = \\sigma(Z^{[1]}_2) \\\\\n",
    "    a^{[1]}_3 = \\sigma(Z^{[1]}_3) \\\\\n",
    "    a^{[1]}_4 = \\sigma(Z^{[1]}_4) \n",
    "    \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " When we're vectorizing, one of the rules of thumb that might help you navigate this, is that while we have different nodes in the layer, we'll stack them vertically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Z^{[1]} = W^{[1]} X + b^{[1]} = \\begin{bmatrix} ---w^{[1]T}_1---\\\\ ---w^{[1]T}_2--- \\\\ ---w^{[1]T}_3--- \\\\ ---w^{[1]T}_4---\\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3  \\end{bmatrix} \n",
    "+ \\begin{bmatrix} b^{[1]T}_1\\\\ b^{[1]T}_2 \\\\ b^{[1]T}_3 \\\\ b^{[1]T}_4 \\end{bmatrix} \n",
    "= \\begin{bmatrix} Z^{[1]}_1\\\\ Z^{[1]}_2 \\\\ Z^{[1]}_3 \\\\ Z^{[1]}_4\\end{bmatrix}$$\n",
    "\n",
    "$$ a^{[1]} = \\sigma(Z^{[1]})= \\begin{bmatrix} a^{[1]}_1\\\\ a^{[1]}_2 \\\\ a^{[1]}_3 \\\\ a^{[1]}_4\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have complete the computation of the hidden layer. Now we need to realize the same process and the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> Output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03-shallow neural network/output-node.PNG\" width = \"300px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is only one node in the output layer, so the notation will be:\n",
    "$$W^{[2]} =\\begin{bmatrix} w_{21} \\\\ w_{22} \\\\ w_{23}  \\end{bmatrix}^T = \\begin{bmatrix} w_{21} & w_{22} & w_{23}  \\end{bmatrix} $$\n",
    "$$ Z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}$$ \n",
    "\n",
    "$$ a^{[2]} = \\sigma(Z^{[2]}) $$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network with one hidden layer -  Equations for one example:\n",
    "$$Z^{[1]} = W^{[1]} X + b^{[1]} $$\n",
    "\n",
    "$$ a^{[1]} = \\sigma(Z^{[1]})$$\n",
    "\n",
    "$$ Z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}$$ \n",
    "\n",
    "$$ a^{[2]} = \\sigma(Z^{[2]}) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Vectorizing across multiple examples <a class=\"anchor\" id=\"section_1_3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to vectorize the previous equations of our 2 layers neuron network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We consider an input X with m examples. Each example have n features :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X=\\begin{bmatrix} .. & .. & .. & ..\\\\ .. & .. & .. & .. \\\\ X^{(1)} & X^{(2)} & .. & X^{(m)}  \\\\.. & .. & .. & ..\\\\ .. & .. & .. & .. \\end{bmatrix} \\in (n \\times m)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So if we consider a hidden layer with k nodes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$ W^{[1]} = \\begin{bmatrix} ---w^{[1]T}_1---\\\\ ---w^{[1]T}_2--- \\\\ ... \\\\ ---w^{[1]T}_{k-1}--- \\\\ ---w^{[1]T}_{k}---\\end{bmatrix} \\in (k \\times n) $$ \n",
    "\n",
    " $$ b^{[1]} = \\begin{bmatrix} b^{[1]T}_1\\\\ b^{[1]T}_2\\\\ ... \\\\ b^{[1]T}_{k-1} \\\\ b^{[1]T}_{k}\\end{bmatrix}  \\in (k \\times 1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result for the hidden layer will be: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Z^{[1]} = W^{[1]} X + b^{[1]} =\\begin{bmatrix} h_{11} & .. & .. & h_{1m}\\\\ .. & .. & .. & .. \\\\ Z^{[1](1)} & Z^{[1](2)} & .. & Z^{[1](m)}   \\\\.. & .. & .. & ..\\\\ .. & .. & .. & .. \\end{bmatrix} \\in (k \\times m)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vertically we have the hidden units\n",
    "- horizontally we have the training examples\n",
    "\n",
    "    - h11 is the first hidden unit (node 1) of the first example\n",
    "    - h1m is the first hidden unit (node 1) of the m-last example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the activation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$a^{[1]} = \\sigma(Z^{[1]})=\\begin{bmatrix} .. & .. & .. & ..\\\\ .. & .. & .. & .. \\\\ a^{[1](1)} & a^{[1](2)} & .. & a^{[1](m)}   \\\\.. & .. & .. & ..\\\\ .. & .. & .. & .. \\end{bmatrix} \\in (k \\times m)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now we do the same process on the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output layer has one node so:\n",
    "\n",
    "$$ W^{[2]} = \\begin{bmatrix} w^{[2]}_{1} & .. & .. & .. & w^{[2]}_{k} \\end{bmatrix} \\in (1 \\times k) $$ \n",
    "\n",
    "$$Z^{[2]} = W^{[2]} a^{[1]} + b^{[2]} =\\begin{bmatrix} w^{[2]}_{1} & .. & .. & .. & w^{[2]}_{k} \\end{bmatrix} \n",
    "\\begin{bmatrix} .. & .. & .. & ..\\\\ .. & .. & .. & .. \\\\ a^{[1](1)} & a^{[1](2)} & .. & a^{[1](m)}   \\\\.. & .. & .. & ..\\\\ .. & .. & .. & .. \\end{bmatrix}  + b^{[2]}$$\n",
    "\n",
    "$$Z^{[2]} =\\begin{bmatrix} Z^{[2]}_{1} & .. & .. & .. & Z^{[2]}_{k} \\end{bmatrix} $$\n",
    "\n",
    "$$ a^{[2]} = \\sigma(Z^{[2]}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Activation Functions <a class=\"anchor\" id=\"chapter2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Overview of the Activation Functions <a class=\"anchor\" id=\"section_2_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you build a Neural network, one of the choices you get to make is what activation function to use in the hidden layers and the output units. \n",
    "\n",
    "So far, we've just have been using the sigmoid activation function, but sometimes other choices can work much better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Sigmoid function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03-shallow neural network/sigmoid_function.PNG\" width = \"400px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> tanh activation function:\n",
    "\n",
    "An activation function that almost always work better than the sigmoid function is the hyperbolic tangent function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03-shallow neural network/tanh_function.PNG\" width = \"300px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of the tanh function are between minus 1 and plus one. For the hidden units it turns out the tanh function almost always work better than the sigmoid function because the mean of the activations that come out of the hidden layer are closer to having a zero mean. When you train a learning algorithm, you might center the data and have your data have zero mean using a tan h instead of a sigmoid function. Kind of has the effect of centering your data so that the mean of your data is close to zero rather than maybe 0.5. And this actually makes learning for the next layer a little bit easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "- For a binary classification, you use the tanh function for the hidden layer. But for the output layer you prefer to use the sigmoid function into order to have y_pred values between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>One of the downsides of both the sigmoid function and the tan h function</b> is that if z is either very large or very small, then the gradient of the derivative of the slope of this function becomes very small. So if z is very large or z is very small, the slope of the function either ends up being close to zero and so this can slow down gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Relu function: Rectified Linear Unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03-shallow neural network/relu_function.PNG\" width = \"400px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the advantage of the <b>reLU function</b> is the derivative of the activation function, the slope of the activation function is very different from zero. And so in practice, using the reLU activation function, your neural network will often <b>learn much faster</b> than when using the tan h or the sigmoid activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Leaky reLU activation function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03-shallow neural network/leaky_relu_function.PNG\" width = \"400px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Why do you need Non-linear activation functions ? <a class=\"anchor\" id=\"section_2_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that if you use a linear activation function or alternatively, if you don't have an activation function, then no matter how many layers your neural network has, all it's doing is just computing a linear activation function. So you might as well not have any hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Derivatives of activation functions <a class=\"anchor\" id=\"section_2_3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you implement backpropagation for your neural network, you need to either compute the slope or the derivative of the activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Derivative of sigmoid :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ g(z) = \\frac{1}{1+\\exp^{-z}}$$\n",
    "\n",
    "$$ \\frac{\\partial g}{\\partial z} = \\frac{\\exp^{-z}}{(1+\\exp^{-z})^2} = g(z)(1-g(z))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Derivative of tanh :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ g(z) = tanh = \\frac{\\exp^{z}-\\exp^{-z}}{\\exp^{z}+\\exp^{-z}}$$\n",
    "\n",
    "$$ \\frac{\\partial g}{\\partial z} = 1 - \\frac{(\\exp^{z}-\\exp^{-z})^2}{(\\exp^{z}+\\exp^{-z})^2} = 1 - g(z)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Derivative of reLU :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ g(z) = max(0,z) $$\n",
    "$$\\frac{\\partial g}{\\partial z} = \\begin{cases}\n",
    "                                    0 \\ \\ if \\ \\ z<0 \\\\\n",
    "                                    1 \\ \\ if \\ \\ z>0\n",
    "\n",
    "                                  \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Derivative of leaky reLU :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ g(z) = max(0.01z,z) $$\n",
    "$$\\frac{\\partial g}{\\partial z} = \\begin{cases}\n",
    "                                    0.01 \\ \\ if \\ \\ z<0 \\\\\n",
    "                                    1 \\ \\ if \\ \\ z>0\n",
    "\n",
    "                                  \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Descent for one hidden layer Neural network <a class=\"anchor\" id=\"chapter3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03-shallow neural network/one-hidden-layer.png\" width = \"400px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Input nx features, m examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X = \\begin{bmatrix}  X^{(1)} &  X^{(2)} & .. & .. &  X^{(m)} \\end{bmatrix}  \\in  \\mathbb{R^{n_X \\times m}}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Parameters and Loss function:\n",
    "\n",
    "- n[2] = 1 (-> 1 output unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ W^{[1]} \\in (n_{[1]} \\times n_X) $$ \n",
    "$$ b^{[1]} \\in (n_{[1]} \\times 1) $$ \n",
    "$$ W^{[2]} \\in (n_{[2]} \\times n_{[1]}) $$ \n",
    "$$ b^{[2]} \\in (n_{[2]} \\times 1) $$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> FORWARD PROPAGATION : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{cases}\n",
    "    Z^{[1]} = W^{[1]} X + b^{[1]} \\\\\n",
    "    A^{[1]} = g^{[1]}(Z^{[1]})  \\\\\n",
    "    Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]} \\\\\n",
    "    A^{[2]} = g^{[2]}(Z^{[2]})\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BACKWARD PROPAGATION :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{cases}\n",
    "    dZ^{[2]} =  (A^{[2]} - Y) \\\\\n",
    "    dW^{[2]} = \\frac{1}{m} (A^{[2]} - Y)A^{[1]T} \\\\\n",
    "    db^{[2]} = \\frac{1}{m} \\sum (A^{[2]} - Y)    \\\\\n",
    "    dZ^{[1]} = W^{[2]T}dZ^{[2]} * g^{[1]'}(Z^{[1]}) \\\\\n",
    "    dW^{[1]} = \\frac{1}{m} dZ^{[1]} X^T\\\\\n",
    "    db^{[1]} = \\frac{1}{m} \\sum  dZ^{[1]}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Random initialization <a class=\"anchor\" id=\"chapter4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you initialize the neural network  equal to 0, then this hidden unit and this hidden unit are completely identical. Sometimes you say they're completely symmetric, which just means that they're completing exactly the same function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> so long as w is initialized randomly, you start off with the different hidden units computing different things. And so you no longer have this symmetry breaking problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Recap <a class=\"anchor\" id=\"chapter5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The tanh activation is almost always better than sigmoid activation function for hidden units because the mean of its output is closer to zero, and so it centers the data, making learning complex for the next layer.\n",
    "\n",
    "- Sigmoid outputs a value between 0 and 1 which makes it a very good choice for binary classification. You can classify as 0 if the output is less than 0.5 and classify as 1 if the output is more than 0.5.\n",
    "\n",
    "- Suppose you have built a neural network. You decide to initialize the weights and biases to be zero : \n",
    "    - Each neuron in the first hidden layer will perform the same computation. So even after multiple iterations of gradient descent each neuron in the layer will be computing the same thing as other neurons. \n",
    "\n",
    "\n",
    "- Logistic Regression doesn't have a hidden layer. If you initialize the weights to zeros, the first example x fed in the logistic regression will output zero but the derivatives of the Logistic Regression depend on the input x (because there's no hidden layer) which is not zero. So at the second iteration, the weights values follow x's distribution and are different from each other if x is not a constant vector. \n",
    "\n",
    "\n",
    "- You have built a network using the tanh activation for all the hidden units. You initialize the weights to relative large values, using np.random.randn(..,..)*1000. What will happen? \n",
    "    - This will cause the inputs of the tanh to also be very large, thus causing gradients to be close to zero. The optimization algorithm will thus become slow."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "247ab06e135bb35fa78c5eff31b2a9a0050dcb5fb773c2631d2a29ac689eeccb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
