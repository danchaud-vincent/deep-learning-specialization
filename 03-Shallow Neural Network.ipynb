{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shallow Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "* [1. Neural Network Representation](#chapter1)\n",
    "    * [1.1 Representation](#section_1_1)\n",
    "    * [1.2  Computing a Neural Network output's](#section_1_2)\n",
    "        * [1.2.1  ](#section_1_2_1)\n",
    "        * [1.2.2 ](#section_1_2_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Neural Network Representation <a class=\"anchor\" id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Representation <a class=\"anchor\" id=\"section_1_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03-shallow neural network/NN-representation.PNG\" width = \"400px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have the input features, x1, x2, x3 stacked up vertically. And this is called the <b>input layer</b> of the neural network.\n",
    "- Then there's another layer of circles. And this is called a <b>hidden layer</b> of the neural network. \n",
    "- The final layer here is formed by, in this case, just one node. And this single-node layer is called the <b>output layer</b>, and is responsible for generating the predicted value\n",
    "\n",
    "> This neural network has <b>Two layers</b>. Indeed we don't count the input layer in a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Computing a Neural Network output's <a class=\"anchor\" id=\"section_1_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A node in a layer does two steps of computation:\n",
    "\n",
    "- 1: It computes z = w.T x + b\n",
    "- 2: It computes the activation function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03-shallow neural network/single-node.PNG\" width = \"300px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So each node in neural network and so in the hidden layer will compute these two steps.\n",
    "\n",
    "By convention the notation are the following:\n",
    "\n",
    "$$ a^{[l]}_i $$\n",
    "- a represents the activation of the layer (input layer a[0] | hidden layer a[1] | output layer a[2])\n",
    "- l means the layer l\n",
    "- i means the node i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Lets compute the activation of the first node in the hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03-shallow neural network/hidden-layer-node1.PNG\" width = \"300px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The representation of X and W are:\n",
    "$$X =\\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3  \\end{bmatrix} $$\n",
    "$$W^{[1]}_1 =\\begin{bmatrix} w_{11} \\\\ w_{12} \\\\ w_{13}  \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two steps to compute in this node:\n",
    "\n",
    "$$ Z^{[1]}_1 = W^{[1]T}_1 X + b^{[1]}_1$$\n",
    "$$ a^{[1]}_1 = \\sigma(Z^{[1]}_1) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We repeat this method on each node on the hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03-shallow neural network/hidden-layer-node2.PNG\" width = \"300px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "    \\begin{cases}\n",
    "    Z^{[1]}_1 = W^{[1]T}_1 X + b^{[1]}_1 \\\\\n",
    "    Z^{[1]}_2 = W^{[1]T}_2 X + b^{[1]}_2 \\\\\n",
    "    Z^{[1]}_3 = W^{[1]T}_3 X + b^{[1]}_3 \\\\\n",
    "    Z^{[1]}_4 = W^{[1]T}_4 X + b^{[1]}_4 \n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "$$ \n",
    "    \\begin{cases}\n",
    "    a^{[1]}_1 = \\sigma(Z^{[1]}_1) \\\\\n",
    "    a^{[1]}_2 = \\sigma(Z^{[1]}_2) \\\\\n",
    "    a^{[1]}_3 = \\sigma(Z^{[1]}_3) \\\\\n",
    "    a^{[1]}_4 = \\sigma(Z^{[1]}_4) \n",
    "    \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " When we're vectorizing, one of the rules of thumb that might help you navigate this, is that while we have different nodes in the layer, we'll stack them vertically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Z^{[1]} = W^{[1]} X + b^{[1]} = \\begin{bmatrix} ---w^{[1]T}_1---\\\\ ---w^{[1]T}_2--- \\\\ ---w^{[1]T}_3--- \\\\ ---w^{[1]T}_4---\\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3  \\end{bmatrix} \n",
    "+ \\begin{bmatrix} b^{[1]T}_1\\\\ b^{[1]T}_2 \\\\ b^{[1]T}_3 \\\\ b^{[1]T}_4 \\end{bmatrix} \n",
    "= \\begin{bmatrix} Z^{[1]}_1\\\\ Z^{[1]}_2 \\\\ Z^{[1]}_3 \\\\ Z^{[1]}_4\\end{bmatrix}$$\n",
    "\n",
    "$$ a^{[1]} = \\sigma(Z^{[1]})= \\begin{bmatrix} a^{[1]}_1\\\\ a^{[1]}_2 \\\\ a^{[1]}_3 \\\\ a^{[1]}_4\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have complete the computation of the hidden layer. Now we need to realize the same process and the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> Output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03-shallow neural network/output-node.PNG\" width = \"300px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is only one node in the output layer, so the notation will be:\n",
    "$$W^{[2]} =\\begin{bmatrix} w_{21} \\\\ w_{22} \\\\ w_{23}  \\end{bmatrix}^T = \\begin{bmatrix} w_{21} & w_{22} & w_{23}  \\end{bmatrix} $$\n",
    "$$ Z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}$$ \n",
    "\n",
    "$$ a^{[2]} = \\sigma(Z^{[2]}) $$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network with one hidden layer -  Equations for one example:\n",
    "$$Z^{[1]} = W^{[1]} X + b^{[1]} $$\n",
    "\n",
    "$$ a^{[1]} = \\sigma(Z^{[1]})$$\n",
    "\n",
    "$$ Z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}$$ \n",
    "\n",
    "$$ a^{[2]} = \\sigma(Z^{[2]}) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Vectorizing across multiple examples <a class=\"anchor\" id=\"section_1_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to vectorize the previous equations of our 2 layers neuron network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We consider an input X with m examples. Each example have n features :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X=\\begin{bmatrix} .. & .. & .. & ..\\\\ .. & .. & .. & .. \\\\ X^{(1)} & X^{(2)} & .. & X^{(m)}  \\\\.. & .. & .. & ..\\\\ .. & .. & .. & .. \\end{bmatrix} \\in (n \\times m)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So if we consider a hidden layer with k nodes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$ W^{[1]} = \\begin{bmatrix} ---w^{[1]T}_1---\\\\ ---w^{[1]T}_2--- \\\\ ... \\\\ ---w^{[1]T}_{k-1}--- \\\\ ---w^{[1]T}_{k}---\\end{bmatrix} \\in (k \\times n) $$ \n",
    "\n",
    " $$ b^{[1]} = \\begin{bmatrix} b^{[1]T}_1\\\\ b^{[1]T}_2\\\\ ... \\\\ b^{[1]T}_{k-1} \\\\ b^{[1]T}_{k}\\end{bmatrix}  \\in (k \\times 1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result for the hidden layer will be: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Z^{[1]} = W^{[1]} X + b^{[1]} =\\begin{bmatrix} h_{11} & .. & .. & h_{1m}\\\\ .. & .. & .. & .. \\\\ Z^{[1](1)} & Z^{[1](2)} & .. & Z^{[1](m)}   \\\\.. & .. & .. & ..\\\\ .. & .. & .. & .. \\end{bmatrix} \\in (k \\times m)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vertically we have the hidden units\n",
    "- horizontally we have the training examples\n",
    "\n",
    "    - h11 is the first hidden unit (node 1) of the first example\n",
    "    - h1m is the first hidden unit (node 1) of the m-last example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the activation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$a^{[1]} = \\sigma(Z^{[1]})=\\begin{bmatrix} .. & .. & .. & ..\\\\ .. & .. & .. & .. \\\\ a^{[1](1)} & a^{[1](2)} & .. & a^{[1](m)}   \\\\.. & .. & .. & ..\\\\ .. & .. & .. & .. \\end{bmatrix} \\in (k \\times m)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
