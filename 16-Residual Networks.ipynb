{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ResNet Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very deep neural networks are difficult to train, because of vanishing and exploding gradient types of problems. \n",
    "\n",
    "Residual Networks are using <b>skip connections</b> which allows us to take the activation from one layer and suddenly feed it to another layer even much deeper in the neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNets are built out of something called a residual block, let's first describe what that is.\n",
    "\n",
    "\n",
    "Here is a two layers of neural network:\n",
    "\n",
    "<center><img src=\"images/11-CNN/residualblock.PNG\"></center>\n",
    "\n",
    "We have the following steps :\n",
    "\n",
    "- start off with activaiton $a^{[l]}$\n",
    "- then we applied the linear operator\n",
    "- After that you apply the ReLU nonlinearity to get $a^{[l+1]}$\n",
    "- Then in the next layer you apply the linear step again\n",
    "- And finally you apply another ReLU to get $a^{[l+2]}$\n",
    "\n",
    "Following equations :\n",
    "\n",
    "- $z^{[l+1]} = W^{[l+1]} a^{[l]} + b^{[l+1]}$\n",
    "- $a^{[l+1]} = g(z^{[l+1]})$\n",
    "- $z^{[l+2]} = W^{[l+2]} a^{[l+1]} + b^{[l+2]}$\n",
    "- $a^{[l+2]} = g(z^{[l+2]})$\n",
    "\n",
    "\n",
    "So in other words, for information from a[l] to flow to a[l+2] it needs to go through all of these steps which is the \"main path\"\n",
    "\n",
    "\n",
    "In Residual Net, we are going to take a[l] and apply it further into the neural network.\n",
    "<center><img src=\"images/11-CNN/skipconnection.PNG\"></center>\n",
    "\n",
    "So the last equation goes away and we instead have the following output a[l+2]:\n",
    "- $a^{[l+2]} = g(z^{[l+2]} + a^{[l]})$\n",
    "\n",
    "<b>Skip connection</b> refers to the activation a[l] just skipping over a layer in order to process information deeper into the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
