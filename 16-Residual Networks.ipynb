{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ResNet Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very deep neural networks are difficult to train, because of vanishing and exploding gradient types of problems. \n",
    "\n",
    "Residual Networks are using <b>skip connections</b> which allows us to take the activation from one layer and suddenly feed it to another layer even much deeper in the neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNets are built out of something called a residual block, let's first describe what that is.\n",
    "\n",
    "\n",
    "Here is a two layers of neural network:\n",
    "\n",
    "<center><img src=\"images/11-CNN/residualblock.PNG\"></center>\n",
    "\n",
    "We have the following steps :\n",
    "\n",
    "- start off with activaiton $a^{[l]}$\n",
    "- then we applied the linear operator\n",
    "- After that you apply the ReLU nonlinearity to get $a^{[l+1]}$\n",
    "- Then in the next layer you apply the linear step again\n",
    "- And finally you apply another ReLU to get $a^{[l+2]}$\n",
    "\n",
    "Following equations :\n",
    "\n",
    "- $z^{[l+1]} = W^{[l+1]} a^{[l]} + b^{[l+1]}$\n",
    "- $a^{[l+1]} = g(z^{[l+1]})$\n",
    "- $z^{[l+2]} = W^{[l+2]} a^{[l+1]} + b^{[l+2]}$\n",
    "- $a^{[l+2]} = g(z^{[l+2]})$\n",
    "\n",
    "\n",
    "So in other words, for information from a[l] to flow to a[l+2] it needs to go through all of these steps which is the \"main path\"\n",
    "\n",
    "\n",
    "In Residual Net, we are going to take a[l] and apply it further into the neural network.\n",
    "<center><img src=\"images/11-CNN/skipconnection.PNG\"></center>\n",
    "\n",
    "So the last equation goes away and we instead have the following output a[l+2]:\n",
    "- $a^{[l+2]} = g(z^{[l+2]} + a^{[l]})$\n",
    "\n",
    "<b>Skip connection</b> refers to the activation a[l] just skipping over a layer in order to process information deeper into the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using residual blocks allows us to train much deeper neural networks. A way to build a ResNet is by taking many of these Residual blocks, and stacking them together to form a deep network.\n",
    "\n",
    "<center><img src=\"images/11-CNN/residualblock2.PNG\"></br><u><em>Figure: Residual Block</em></u>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference between a plain network and a Resnet:\n",
    "\n",
    "<center><img src=\"images/11-CNN/plainnetwork.PNG\"></br><u><em>Figure: Plain Network</em></u>\n",
    "</center>\n",
    "<br>\n",
    "<center><img src=\"images/11-CNN/resnet.PNG\"></br><u><em>Figure: Residual Network</em></u>\n",
    "</center>\n",
    "\n",
    "Empirically, you find that as you increase the number of layers, the training error will tend to decreaser after a while but then they'll tend to go back up. And in theory as you make a neural network deeper it should only do better and better on the training set. So having a deeper network should only help. But in practice or in reality, having <b>a plain network</b> that is very deep means that all your optimization algorithm just has a much harder time training. And so in reality your training error gets worse if you pick a network that's too deep.<br>\n",
    "But what happens with ResNet is that even as the number of layers gets deeper, you can have the performance of the training error king of keep going down. \n",
    "\n",
    "<center><img src=\"images/11-CNN/trainingerror.PNG\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Why ResNet Works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
