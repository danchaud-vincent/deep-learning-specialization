{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an Optimization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [1. MiniBatch Neural Network Model](#chapter1)\n",
    "    * [1.1 Initialize parameters](#section_1_1)\n",
    "    * [1.2  Forward propagation](#section_1_2)\n",
    "    * [1.3  Compute Cost](#section_1_3)\n",
    "    * [1.4  Backward propagation](#section_1_4)\n",
    "    * [1.5  Minibatch](#section_1_5)\n",
    "* [2. Optimization Algorithms](#chapter2)\n",
    "    * [2.1 Without optimization](#section_2_1)\n",
    "    * [2.2 Momentum optimization](#section_2_2)\n",
    "    * [2.3 Adam optimization](#section_2_3)\n",
    "* [3. Model](#chapter3)\n",
    "    * [3.1 Model built](#section_3_1)\n",
    "    * [3.2 Test on dataset](#section_3_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Build a neural network model with Minibatch <a class=\"anchor\" id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Initialize parameters <a class=\"anchor\" id=\"section_1_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layers_dims,n_input,n_output):\n",
    "    \"\"\"\n",
    "    Compute the initialization of the parameters in our Neural Network\n",
    "\n",
    "    - Arguments:\n",
    "    layers_dims: array containing the dimension of the hidden layers\n",
    "    n_input: numbers of features in the input layer\n",
    "    n_ouput: numbers of nodes in the output layer\n",
    "    type_init: \"zeros\",\"random\",\"he\" type of initialization\n",
    "\n",
    "    - Return:\n",
    "    parameters: dictionnary containing of the parameters of our neural network\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    # init\n",
    "    parameters = {}\n",
    "\n",
    "    # add the output layer to the array\n",
    "    layers_dims.append(n_output)\n",
    "\n",
    "    # number of layers\n",
    "    L = len(layers_dims)\n",
    "\n",
    "    for i in range(L):\n",
    "        \n",
    "        # if i==0 take n_x features\n",
    "        if i ==0:\n",
    "            layer_prev = n_input\n",
    "        else:\n",
    "            layer_prev = layers_dims[i-1]\n",
    "\n",
    "        # default init\n",
    "        parameters[\"W\" + str(i+1)] = np.random.randn(layers_dims[i],layer_prev) * np.sqrt(2/layer_prev)\n",
    "        parameters[\"b\" + str(i+1)] = np.zeros((layers_dims[i],1))\n",
    "\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Forward propagation <a class=\"anchor\" id=\"section_1_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_function(Z,activation_name):\n",
    "\n",
    "    if activation_name.lower() == \"sigmoid\":\n",
    "\n",
    "        A = 1/(1+np.exp(-Z))\n",
    "    \n",
    "    elif activation_name.lower() == \"relu\":\n",
    "\n",
    "        A = np.maximum(0,Z)\n",
    "\n",
    "    elif activation_name.lower() == \"tanh\":\n",
    "\n",
    "        A = np.tanh(Z)\n",
    "    \n",
    "    else:\n",
    "        # By default relu\n",
    "        A = np.maximum(0,Z)\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X,parameters,activation_name=\"relu\"):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute the activation function\n",
    "    \n",
    "    Arguments:\n",
    "    activation_name -- name of the activation function choosen\n",
    "    Z -- items\n",
    "\n",
    "    Returns:\n",
    "    activation -- activation value\n",
    "    \"\"\"\n",
    "    # init cache\n",
    "    caches = []\n",
    "    cache_layer = {}\n",
    "\n",
    "    # layer\n",
    "    L = len(parameters)//2\n",
    "\n",
    "    # setting A_prev to X\n",
    "    A_prev = X\n",
    "\n",
    "    for i in range(1,L+1):\n",
    "\n",
    "        # getting parameters\n",
    "        W = parameters[\"W\" + str(i)]\n",
    "        b= parameters[\"b\" + str(i)]\n",
    "\n",
    "        # linear result\n",
    "        Z = np.dot(W,A_prev) + b\n",
    "\n",
    "        if i==L:\n",
    "            # last layer -  sigmoid \n",
    "            A = activation_function(Z,\"sigmoid\")\n",
    "        else:\n",
    "            A = activation_function(Z,\"relu\")\n",
    "\n",
    "        # adding to the cache\n",
    "        cache = {\"W\" : W, \"b\":b,\"A\":A,\"Z\":Z,\"A_prev\": A_prev}\n",
    "\n",
    "        # adding layer cache\n",
    "        caches.append(cache)\n",
    "\n",
    "        # setting A_prev\n",
    "        A_prev = A\n",
    "\n",
    "    return A, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Compute cost<a class=\"anchor\" id=\"section_1_3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(AL,y):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute the log loss L(y_pred,y)\n",
    "\n",
    "    -- Arguments:\n",
    "    y : true labels of the dataset\n",
    "    AL : result of the forward propagation \n",
    "\n",
    "    -- Returns:\n",
    "    cost : Log loss cost\n",
    "\n",
    "    \"\"\"\n",
    "    # m examples\n",
    "    m = y.shape[1]\n",
    "\n",
    "    epsilon = 1e-15\n",
    "\n",
    "    cost = (np.multiply(y,np.log(AL+epsilon)) + np.multiply(1-y,np.log(1-AL+epsilon)))\n",
    "    cost = -(1/m) *np.nansum(cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Backward propagation  <a class=\"anchor\" id=\"section_1_4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_activation(dA,Z,function_name=\"relu\"):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute dZ for the backward propagation\n",
    "\n",
    "    -- Arguments:\n",
    "    dA : derivative of A\n",
    "    Z : linear activation\n",
    "    function_name: name of the activation_function\n",
    "\n",
    "    -- Returns:\n",
    "    dZ: derivative of Z\n",
    "    \"\"\"\n",
    "\n",
    "    if function_name.lower() == \"sigmoid\":\n",
    "        # sigmoid\n",
    "        s = 1/(1+np.exp(-Z))\n",
    "\n",
    "        # derivative sigmoid\n",
    "        dG = s*(1-s)\n",
    "\n",
    "        # dZ\n",
    "        dZ = dA * dG\n",
    "\n",
    "    elif function_name.lower() == \"relu\":\n",
    "\n",
    "        # relu\n",
    "        r = np.maximum(0,Z)\n",
    "\n",
    "        # derivative relu\n",
    "        dG = np.int64(r>0)\n",
    "\n",
    "        # dZ\n",
    "        dZ = np.multiply(dA,dG)\n",
    "\n",
    "    elif function_name.lower() == \"tanh\":\n",
    "\n",
    "        # tanh\n",
    "        th = np.tanh(Z)\n",
    "\n",
    "        # derivative tanh\n",
    "        dG = 1-np.power(th,2)\n",
    "\n",
    "        # dZ\n",
    "        dZ = dA * dG\n",
    "\n",
    "    else:\n",
    "        # by default relu\n",
    "        r = np.maximum(0,Z)\n",
    "\n",
    "        # derivative relu\n",
    "        dG = np.int64(r>0)\n",
    "\n",
    "        # dZ\n",
    "        dZ = np.multiply(dA,dG)\n",
    "\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(AL,y,caches,function_name=\"relu\"):\n",
    "\n",
    "    # gradients\n",
    "    gradients = {}\n",
    "\n",
    "    # numbers of layers\n",
    "    L = len(caches)\n",
    "\n",
    "    # number of examples\n",
    "    m = y.shape[1]\n",
    "\n",
    "    # dAL\n",
    "    dAL = - (np.divide(y, AL) - np.divide(1 - y, 1 - AL))\n",
    "    \n",
    "    # getting cache layer L\n",
    "    current_cache = caches[-1]\n",
    "    WL = current_cache['W']\n",
    "    ZL = current_cache['Z']\n",
    "    A_prev = current_cache['A_prev']\n",
    "\n",
    "    dZL = backward_activation(dAL,ZL,\"sigmoid\")\n",
    "\n",
    "    dW_temp = (1/m)*np.dot(dZL,A_prev.T)\n",
    "    db_temp = (1/m) * np.sum(dZL,axis=1, keepdims=True)\n",
    "    dA_prev_temp = np.dot(WL.T,dZL)\n",
    "    \n",
    "    # compute the gradient\n",
    "    gradients[\"dW\" + str(L)] = dW_temp\n",
    "    gradients[\"db\" + str(L)] = db_temp\n",
    "\n",
    "\n",
    "    for i in reversed(range(L-1)):\n",
    "\n",
    "        # getting cache layer L\n",
    "        current_cache = caches[i]\n",
    "        W = current_cache['W']\n",
    "        Z = current_cache['Z']\n",
    "        A_prev = current_cache['A_prev']\n",
    "\n",
    "        dZ = backward_activation(dA_prev_temp,Z,function_name)\n",
    "\n",
    "        dW_temp = (1/m)*np.dot(dZ,A_prev.T)\n",
    "        db_temp = (1/m) * np.sum(dZ,axis=1, keepdims=True)\n",
    "        dA_prev_temp = np.dot(W.T,dZ)\n",
    "        \n",
    "        # compute the gradient\n",
    "        gradients[\"dW\" + str(i+1)] = dW_temp\n",
    "        gradients[\"db\" + str(i+1)] = db_temp\n",
    "\n",
    "\n",
    "    return gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Minibatch <a class=\"anchor\" id=\"section_1_5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use random minibatches to accelerate convergence and improve optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Optimization Algorithms <a class=\"anchor\" id=\"chapter2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Without optimization algorithm  <a class=\"anchor\" id=\"section_2_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters,gradients,learning_rate):\n",
    "\n",
    "    # copy \n",
    "    params = copy.deepcopy(parameters)\n",
    "    \n",
    "    # nb layer\n",
    "    L = len(parameters)//2\n",
    "\n",
    "    for i in range(L):\n",
    "        params[\"W\" + str(i+1)] = params[\"W\" + str(i+1)] - learning_rate * gradients[\"dW\" + str(i+1)]\n",
    "        params[\"b\" + str(i+1)] = params[\"b\" + str(i+1)] - learning_rate * gradients[\"db\" + str(i+1)]\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Momentum  <a class=\"anchor\" id=\"section_2_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because mini-batch gradient descent makes a parameter update after seeing just a subset of examples, the direction of the update has some variance, and so the path taken by mini-batch gradient descent will \"oscillate\" toward convergence. Using momentum can reduce these oscillations. \n",
    "\n",
    "Momentum takes into account the past gradients to smooth out the update. The 'direction' of the previous gradients is stored in the variable $v$. Formally, this will be the exponentially weighted average of the gradient on previous steps. You can also think of $v$ as the \"velocity\" of a ball rolling downhill, building up speed (and momentum) according to the direction of the gradient/slope of the hill. \n",
    "\n",
    "\n",
    "$$ \\begin{cases}\n",
    "v_{dW^{[l]}} = \\beta v_{dW^{[l]}} + (1 - \\beta) dW^{[l]} \\\\\n",
    "W^{[l]} = W^{[l]} - \\alpha v_{dW^{[l]}}\n",
    "\\end{cases}\\tag{3}$$\n",
    "\n",
    "$$\\begin{cases}\n",
    "v_{db^{[l]}} = \\beta v_{db^{[l]}} + (1 - \\beta) db^{[l]} \\\\\n",
    "b^{[l]} = b^{[l]} - \\alpha v_{db^{[l]}} \n",
    "\\end{cases}\\tag{4}$$\n",
    "\n",
    "where L is the number of layers, $\\beta$ is the momentum and $\\alpha$ is the learning rate. All parameters should be stored in the `parameters` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Adam  <a class=\"anchor\" id=\"section_2_3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Recap :***\n",
    "\n",
    "Adam is one of the most effective optimization algorithms for training neural networks. It combines ideas from RMSProp (described in lecture) and Momentum. \n",
    "\n",
    "**How does Adam work?**\n",
    "1. It calculates an exponentially weighted average of past gradients, and stores it in variables $v$ (before bias correction) and $v^{corrected}$ (with bias correction). \n",
    "2. It calculates an exponentially weighted average of the squares of the past gradients, and  stores it in variables $s$ (before bias correction) and $s^{corrected}$ (with bias correction). \n",
    "3. It updates parameters in a direction based on combining information from \"1\" and \"2\".\n",
    "\n",
    "The update rule is, for $l = 1, ..., L$: \n",
    "\n",
    "$$\\begin{cases}\n",
    "v_{dW^{[l]}} = \\beta_1 v_{dW^{[l]}} + (1 - \\beta_1) \\frac{\\partial \\mathcal{J} }{ \\partial W^{[l]} } \\\\\n",
    "v^{corrected}_{dW^{[l]}} = \\frac{v_{dW^{[l]}}}{1 - (\\beta_1)^t} \\\\\n",
    "s_{dW^{[l]}} = \\beta_2 s_{dW^{[l]}} + (1 - \\beta_2) (\\frac{\\partial \\mathcal{J} }{\\partial W^{[l]} })^2 \\\\\n",
    "s^{corrected}_{dW^{[l]}} = \\frac{s_{dW^{[l]}}}{1 - (\\beta_2)^t} \\\\\n",
    "W^{[l]} = W^{[l]} - \\alpha \\frac{v^{corrected}_{dW^{[l]}}}{\\sqrt{s^{corrected}_{dW^{[l]}}} + \\varepsilon}\n",
    "\\end{cases}$$\n",
    "where:\n",
    "- t counts the number of steps taken of Adam \n",
    "- L is the number of layers\n",
    "- $\\beta_1$ and $\\beta_2$ are hyperparameters that control the two exponentially weighted averages. \n",
    "- $\\alpha$ is the learning rate\n",
    "- $\\varepsilon$ is a very small number to avoid dividing by zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model  <a class=\"anchor\" id=\"chapter3\"></a>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "247ab06e135bb35fa78c5eff31b2a9a0050dcb5fb773c2631d2a29ac689eeccb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
