{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Gradient Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layers_dims,n_input,n_output):\n",
    "    \"\"\"\n",
    "    Compute the initialization of the parameters in our Neural Network\n",
    "\n",
    "    - Arguments:\n",
    "    layers_dims: array containing the dimension of the hidden layers\n",
    "    n_input: numbers of features in the input layer\n",
    "    n_ouput: numbers of nodes in the output layer\n",
    "    type_init: \"zeros\",\"random\",\"he\" type of initialization\n",
    "\n",
    "    - Return:\n",
    "    parameters: dictionnary containing of the parameters of our neural network\n",
    "    \"\"\"\n",
    "    np.random.seed(3)\n",
    "    \n",
    "    # init\n",
    "    parameters = {}\n",
    "\n",
    "    # add the output layer to the array\n",
    "    layers_dims.append(n_output)\n",
    "\n",
    "    # number of layers\n",
    "    L = len(layers_dims)\n",
    "\n",
    "    for i in range(L):\n",
    "        \n",
    "        # if i==0 take n_x features\n",
    "        if i ==0:\n",
    "            layer_prev = n_input\n",
    "        else:\n",
    "            layer_prev = layers_dims[i-1]\n",
    "\n",
    "        # default init\n",
    "        parameters[\"W\" + str(i+1)] = np.random.randn(layers_dims[i],layer_prev) * np.sqrt(2/layer_prev)\n",
    "        parameters[\"b\" + str(i+1)] = np.zeros((layers_dims[i],1))\n",
    "\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_function(Z,activation_name):\n",
    "\n",
    "    if activation_name.lower() == \"sigmoid\":\n",
    "\n",
    "        A = 1/(1+np.exp(-Z))\n",
    "    \n",
    "    elif activation_name.lower() == \"relu\":\n",
    "\n",
    "        A = np.maximum(0,Z)\n",
    "\n",
    "    elif activation_name.lower() == \"tanh\":\n",
    "\n",
    "        A = np.tanh(Z)\n",
    "    \n",
    "    else:\n",
    "        # By default relu\n",
    "        A = np.maximum(0,Z)\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(AL,y):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute the log loss L(y_pred,y)\n",
    "\n",
    "    -- Arguments:\n",
    "    y : true labels of the dataset\n",
    "    AL : result of the forward propagation \n",
    "\n",
    "    -- Returns:\n",
    "    cost : Log loss cost\n",
    "\n",
    "    \"\"\"\n",
    "    # m examples\n",
    "    m = y.shape[1]\n",
    "\n",
    "    epsilon = 1e-15\n",
    "\n",
    "    cost = (np.multiply(y,np.log(AL+epsilon)) + np.multiply(1-y,np.log(1-AL+epsilon)))\n",
    "    cost = -(1/m) *np.nansum(cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X,y,parameters,activation_name=\"relu\"):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute the activation function\n",
    "    \n",
    "    Arguments:\n",
    "    activation_name -- name of the activation function choosen\n",
    "    Z -- items\n",
    "\n",
    "    Returns:\n",
    "    activation -- activation value\n",
    "    \"\"\"\n",
    "    # init cache\n",
    "    caches = []\n",
    "    cache_layer = {}\n",
    "\n",
    "    # layer\n",
    "    L = len(parameters)//2\n",
    "\n",
    "    # setting A_prev to X\n",
    "    A_prev = X\n",
    "\n",
    "    for i in range(1,L+1):\n",
    "\n",
    "        # getting parameters\n",
    "        W = parameters[\"W\" + str(i)]\n",
    "        b= parameters[\"b\" + str(i)]\n",
    "\n",
    "        # linear result\n",
    "        Z = np.dot(W,A_prev) + b\n",
    "\n",
    "        if i==L:\n",
    "            # last layer -  sigmoid \n",
    "            A = activation_function(Z,\"sigmoid\")\n",
    "        else:\n",
    "            A = activation_function(Z,activation_name)\n",
    "\n",
    "        # adding to the cache\n",
    "        cache = {\"W\" : W, \"b\":b,\"A\":A,\"Z\":Z,\"A_prev\": A_prev}\n",
    "\n",
    "        # adding layer cache\n",
    "        caches.append(cache)\n",
    "\n",
    "        # setting A_prev\n",
    "        A_prev = A\n",
    "\n",
    "    cost = cost_function(A,y)\n",
    "\n",
    "    return cost, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_activation(dA,Z,function_name=\"relu\"):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute dZ for the backward propagation\n",
    "\n",
    "    -- Arguments:\n",
    "    dA : derivative of A\n",
    "    Z : linear activation\n",
    "    function_name: name of the activation_function\n",
    "\n",
    "    -- Returns:\n",
    "    dZ: derivative of Z\n",
    "    \"\"\"\n",
    "\n",
    "    if function_name.lower() == \"sigmoid\":\n",
    "        # sigmoid\n",
    "        s = 1/(1+np.exp(-Z))\n",
    "\n",
    "        # derivative sigmoid\n",
    "        dG = s*(1-s)\n",
    "\n",
    "        # dZ\n",
    "        dZ = dA * dG\n",
    "\n",
    "    elif function_name.lower() == \"relu\":\n",
    "\n",
    "        # relu\n",
    "        r = np.maximum(0,Z)\n",
    "\n",
    "        # derivative relu\n",
    "        dG = np.int64(r>0)\n",
    "\n",
    "        # dZ\n",
    "        dZ = np.multiply(dA,dG)\n",
    "\n",
    "    elif function_name.lower() == \"tanh\":\n",
    "\n",
    "        # tanh\n",
    "        th = np.tanh(Z)\n",
    "\n",
    "        # derivative tanh\n",
    "        dG = 1-np.power(th,2)\n",
    "\n",
    "        # dZ\n",
    "        dZ = dA * dG\n",
    "\n",
    "    else:\n",
    "        # by default relu\n",
    "        r = np.maximum(0,Z)\n",
    "\n",
    "        # derivative relu\n",
    "        dG = np.int64(r>0)\n",
    "\n",
    "        # dZ\n",
    "        dZ = np.multiply(dA,dG)\n",
    "\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(AL,y,caches,function_name=\"relu\"):\n",
    "\n",
    "    # gradients\n",
    "    gradients = {}\n",
    "\n",
    "    # numbers of layers\n",
    "    L = len(caches)\n",
    "\n",
    "    # number of examples\n",
    "    m = y.shape[1]\n",
    "\n",
    "    # dAL\n",
    "    dAL = - (np.divide(y, AL) - np.divide(1 - y, 1 - AL))\n",
    "    \n",
    "    # getting cache layer L\n",
    "    current_cache = caches[-1]\n",
    "    WL = current_cache['W']\n",
    "    ZL = current_cache['Z']\n",
    "    A_prev = current_cache['A_prev']\n",
    "\n",
    "    dZL = backward_activation(dAL,ZL,\"sigmoid\")\n",
    "\n",
    "    dW_temp = (1/m)*np.dot(dZL,A_prev.T)\n",
    "    db_temp = (1/m) * np.sum(dZL,axis=1, keepdims=True)\n",
    "    dA_prev_temp = np.dot(WL.T,dZL)\n",
    "    \n",
    "    # compute the gradient\n",
    "    gradients[\"dW\" + str(L)] = dW_temp\n",
    "    gradients[\"db\" + str(L)] = db_temp\n",
    "\n",
    "\n",
    "    for i in reversed(range(L-1)):\n",
    "\n",
    "        # getting cache layer L\n",
    "        current_cache = caches[i]\n",
    "        W = current_cache['W']\n",
    "        Z = current_cache['Z']\n",
    "        A_prev = current_cache['A_prev']\n",
    "\n",
    "        dZ = backward_activation(dA_prev_temp,Z,function_name)\n",
    "\n",
    "        dW_temp = (1/m)*np.dot(dZ,A_prev.T)\n",
    "        db_temp = (1/m) * np.sum(dZ,axis=1, keepdims=True)\n",
    "        dA_prev_temp = np.dot(W.T,dZ)\n",
    "        \n",
    "        # compute the gradient\n",
    "        gradients[\"dW\" + str(i+1)] = dW_temp\n",
    "        gradients[\"db\" + str(i+1)] = db_temp\n",
    "\n",
    "\n",
    "    return gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation computes the gradients $\\frac{\\partial J}{\\partial \\theta}$, where $\\theta$ denotes the parameters of the model. $J$ is computed using forward propagation and your loss function.\n",
    "\n",
    "Because forward propagation is relatively easy to implement, you're confident you got that right, and so you're almost 100% sure that you're computing the cost $J$ correctly. Thus, you can use your code for computing $J$ to verify the code for computing $\\frac{\\partial J}{\\partial \\theta}$.\n",
    "\n",
    "Let's look back at the definition of a derivative (or gradient):$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} $$\n",
    "\n",
    "If you're not familiar with the \"$\\displaystyle \\lim_{\\varepsilon \\to 0}$\" notation, it's just a way of saying \"when $\\varepsilon$ is really, really small.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gradient Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each i in num_parameters:\n",
    "- To compute `J_plus[i]`:\n",
    "    1. Set $\\theta^{+}$ to `np.copy(parameters_values)`\n",
    "    2. Set $\\theta^{+}_i$ to $\\theta^{+}_i + \\varepsilon$\n",
    "    3. Calculate $J^{+}_i$ using to `forward_propagation_n(x, y, vector_to_dictionary(`$\\theta^{+}$ `))`.     \n",
    "- To compute `J_minus[i]`: do the same thing with $\\theta^{-}$\n",
    "- Compute $gradapprox[i] = \\frac{J^{+}_i - J^{-}_i}{2 \\varepsilon}$\n",
    "\n",
    "Thus, you get a vector gradapprox, where gradapprox[i] is an approximation of the gradient with respect to `parameter_values[i]`. You can now compare this gradapprox vector to the gradients vector from backpropagation. Just like for the 1D case (Steps 1', 2', 3'), compute: \n",
    "$$ difference = \\frac {\\| grad - gradapprox \\|_2}{\\| grad \\|_2 + \\| gradapprox \\|_2 } \\tag{3}$$\n",
    "\n",
    "**Note**: Use `np.linalg.norm` to get the norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dictionary_to_vector(parameters):\n",
    "    \"\"\"\n",
    "    Roll all our parameters dictionary into a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    keys = []\n",
    "    count = 0\n",
    "    \n",
    "    for key in parameters.keys():\n",
    "        \n",
    "        # flatten parameter\n",
    "        new_vector = np.reshape(parameters[key], (-1, 1))\n",
    "        \n",
    "        # save of keys\n",
    "        theta_lim = new_vector.shape[0]\n",
    "        keys.append((key,theta_lim,parameters[key].shape))\n",
    "        \n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concatenate((theta, new_vector), axis=0)\n",
    "        count = count + 1\n",
    "    \n",
    "    return theta, keys\n",
    "\n",
    "def vector_to_dictionary(theta,keys):\n",
    "    \"\"\"\n",
    "    Unroll all our parameters dictionary from a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    theta_lim_prev = 0\n",
    "\n",
    "    for key, theta_lim,param_shape in keys:\n",
    "\n",
    "        # limite theta\n",
    "        theta_lim_start = theta_lim_prev\n",
    "        theta_lim_end = theta_lim + theta_lim_prev\n",
    "       \n",
    "        parameters[key] = theta[theta_lim_start:theta_lim_end].reshape(param_shape)\n",
    "\n",
    "        theta_lim_prev = theta_lim_end\n",
    "    \n",
    "    \n",
    "    # parameters[\"W1\"] = theta[: 20].reshape((5, 4))\n",
    "    # parameters[\"b1\"] = theta[20: 25].reshape((5, 1))\n",
    "    # parameters[\"W2\"] = theta[25: 40].reshape((3, 5))\n",
    "    # parameters[\"b2\"] = theta[40: 43].reshape((3, 1))\n",
    "    # parameters[\"W3\"] = theta[43: 46].reshape((1, 3))\n",
    "    # parameters[\"b3\"] = theta[46: 47].reshape((1, 1))\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def gradients_to_vector(gradients):\n",
    "    \"\"\"\n",
    "    Roll all our gradients dictionary into a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    \n",
    "    # sort keys gradients like keys parameters\n",
    "    grad_keys = []\n",
    "    for grad_key in gradients.keys():\n",
    "        if grad_key.startswith(\"dW\"):\n",
    "            grad_keys.insert(0,grad_key)\n",
    "        else:\n",
    "            grad_keys.insert(1,grad_key)\n",
    "    count = 0\n",
    "    for key in grad_keys:\n",
    "        # flatten parameter\n",
    "        new_vector = np.reshape(gradients[key], (-1, 1))\n",
    "        \n",
    "        \n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concatenate((theta, new_vector), axis=0)\n",
    "        count = count + 1\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: gradient_check_n\n",
    "\n",
    "def gradient_check_n(parameters, gradients, X, Y, epsilon=1e-7, print_msg=False):\n",
    "    \"\"\"\n",
    "    Checks if backward_propagation_n computes correctly the gradient of the cost output by forward_propagation_n\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
    "    grad -- output of backward_propagation_n, contains gradients of the cost with respect to the parameters. \n",
    "    x -- input datapoint, of shape (input size, 1)\n",
    "    y -- true \"label\"\n",
    "    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)\n",
    "    \n",
    "    Returns:\n",
    "    difference -- difference (2) between the approximated gradient and the backward propagation gradient\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set-up variables\n",
    "    parameters_values, parameters_keys = dictionary_to_vector(parameters)\n",
    "    \n",
    "    grad = gradients_to_vector(gradients)\n",
    "  \n",
    "  \n",
    "    num_parameters = parameters_values.shape[0]\n",
    "    J_plus = np.zeros((num_parameters, 1))\n",
    "    J_minus = np.zeros((num_parameters, 1))\n",
    "    gradapprox = np.zeros((num_parameters, 1))\n",
    "   \n",
    "    # Compute gradapprox\n",
    "    for i in range(num_parameters):\n",
    "        \n",
    "        # Compute J_plus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_plus[i]\".\n",
    "        # \"_\" is used because the function you have to outputs two parameters but we only care about the first one\n",
    "        #(approx. 3 lines)\n",
    "        # theta_plus =                                        # Step 1\n",
    "        # theta_plus[i] =                                     # Step 2\n",
    "        # J_plus[i], _ =                                     # Step 3\n",
    "        # YOUR CODE STARTS HERE\n",
    "        theta_plus = np.copy(parameters_values)\n",
    "        theta_plus[i] = theta_plus[i] + epsilon\n",
    "        J_plus[i],_ = forward_propagation(X, Y, vector_to_dictionary(theta_plus,parameters_keys))\n",
    "        \n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        # Compute J_minus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_minus[i]\".\n",
    "        #(approx. 3 lines)\n",
    "        # theta_minus =                                    # Step 1\n",
    "        # theta_minus[i] =                                 # Step 2        \n",
    "        # J_minus[i], _ =                                 # Step 3\n",
    "        # YOUR CODE STARTS HERE\n",
    "        theta_minus = np.copy(parameters_values)\n",
    "        theta_minus[i] = theta_minus[i] - epsilon\n",
    "        J_minus[i], _ = forward_propagation(X, Y, vector_to_dictionary(theta_minus,parameters_keys))\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        # Compute gradapprox[i]\n",
    "        # (approx. 1 line)\n",
    "        # gradapprox[i] = \n",
    "        # YOUR CODE STARTS HERE\n",
    "        gradapprox[i] = (J_plus[i]-J_minus[i])/(2*epsilon)\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Compare gradapprox to backward propagation gradients by computing difference.\n",
    "    # (approx. 1 line)\n",
    "    # numerator =                                             # Step 1'\n",
    "    # denominator =                                           # Step 2'\n",
    "    # difference =                                            # Step 3'\n",
    "    # YOUR CODE STARTS HERE\n",
    "    numerator = np.linalg.norm(grad - gradapprox)\n",
    "    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)\n",
    "    difference = numerator/denominator\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    if print_msg:\n",
    "        if difference > 2e-7:\n",
    "            print (\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
    "        else:\n",
    "            print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n",
    "\n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check_n_test_case(): \n",
    "    np.random.seed(1)\n",
    "    x = np.random.randn(4,3)\n",
    "    y = np.array([1, 1, 0])\n",
    "    W1 = np.random.randn(5,4) \n",
    "    b1 = np.random.randn(5,1) \n",
    "    W2 = np.random.randn(3,5) \n",
    "    b2 = np.random.randn(3,1) \n",
    "    W3 = np.random.randn(1,3) \n",
    "    b3 = np.random.randn(1,1) \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "\n",
    "    \n",
    "    return x, y, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mYour backward propagation works perfectly fine! difference = 1.189359229166263e-07\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "X, Y, parameters = gradient_check_n_test_case()\n",
    "Y=Y.reshape(1,3)\n",
    "\n",
    "cost, cache = forward_propagation(X, Y, parameters)\n",
    "AL=cache[-1][\"A\"]\n",
    "\n",
    "gradients = backward_propagation(AL, Y, cache)\n",
    "difference = gradient_check_n(parameters, gradients, X, Y, 1e-7, True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d46af94c2bbce495f1e668725902fa517c90b1782bcfe2fce0dd9868df553d3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
