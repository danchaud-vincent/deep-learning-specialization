{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression as a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is an algorithm for binary classification. \n",
    "\n",
    "Here's an example of a binary classification:\n",
    ">You have an input of an image and you want to output a label to recognize this image as either being a cat, in which cas you output 1, or not-cat in which cas you output 0. We use y to denote the output label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/01-logistic-regression/binary-classification.PNG\" width = \"600px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To store an image your computer stores three separete matrices corresponding to the red, green, and blue color channels of this image.\n",
    "\n",
    "Example:\n",
    "- Input image : 64 pixels by 64 pixels.\n",
    "- Then you would have 3  64 by 64 matrices corresponding to the red, blue and green pixel intensity values for your images.\n",
    "- a pixel value is between 0 to 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To turn these pixels into a feature vector, we need to unroll all of these pixel values into a Feature vector X.\n",
    "\n",
    "The number of the pixels and so the dimension of the input features will be n or nx.\n",
    "\n",
    "So for one image we will have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X=\\begin{bmatrix} 255\\\\231\\\\42\\\\...\\\\255\\\\134\\\\...\\\\255\\\\134\\\\...\\end{bmatrix} \\in  \\mathbb{R^{n \\times 1}}$$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y \\in {0,1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we have a training set of m training examples (m images n pixels / m examples n features):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${(X^{(1)},y^{(1)}),(X^{(2)},y^{(2)}),(X^{(3)},y^{(3)}),...,(X^{(m)},y^{(m)})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X=\\begin{bmatrix} . & . & . & . \\\\ . & . & . & . \\\\ X^{(1)} & X^{(2)} & ... & X^{(m)} \\\\. & . & . & .\\\\. & . & . & . \\end{bmatrix} \\in  \\mathbb{R^{n \\times m}}$$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Y \\in  \\mathbb{R^{1 \\times m}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is a learning algorithm that you use when the output labels Y in a supervised learning problem are all either zero or one, so for a binary classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a X vector, corresponding for example to an image\n",
    "> We want an algorithm that can output a prediction called y_pred. y_pred is the probability of the chance of y is equal to 1 given the input features X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y_{pred} = P(y=1|X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The parameters of logistic Regression will be W and b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$W \\in \\mathbb{R^{n \\times 1}} \\ , \\ b \\in \\mathbb{R}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Given the input: $$X \\in \\mathbb{R^{n \\times 1}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output could be y_pred = W T.X + b\n",
    "\n",
    "But if we use this output we are doing a linear regression. But this isn't a very good algorithm for binary classification because we want y_pred to be between zero and one (a probability). Indeed with this calcul can be much bigger than one or it can even be negative, which doesn't make sense for probability. That you want it to be between zero and one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> So in logistic Regression the output Y is equal to the sigmoid function applied to the quantity : W T.X + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma(W^{T} X + b) \\\\ with \\ \\sigma=sigmoid \\ function$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/01-logistic-regression/sigmoid.png\" width = \"500px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Logistic Regression Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">To train the parameters W and b of logistic regresision model. You need to define a cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y_{pred} = \\sigma(W^{T} X + b), \\ where \\ \\sigma(z)= \\frac{1}{1+ \\exp{-z}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn parameters for your model, you are given a training set of m training examples. And on the training set you have also the outputs y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each example in your training set you will have the prediction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y_{pred}^{(i)} = \\sigma(W^{T} X^{(i)} + b), \\ where \\ \\sigma(z^{(i)})= \\frac{1}{1+ \\exp{-z^{(i)}}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i-th example :\n",
    "$$X^{(i)}$$\n",
    "$$y^{(i)}, \\ i-th \\ true \\ label$$\n",
    "$$y_{pred}^{(i)}, \\ i-th \\ prediction$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Loss (error)  function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we can do is use <i>the square erro or one half square error</i> as loss function:\n",
    "$$(Don't \\ use) \\ L(y_{pred},y) = \\frac{1}{2} (y_{pred}-y)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> However  <b>in logistic Regressionwe don't use it</b>. Because when you come to learn the parameters, you find that the optimization problem becomes non convex. We have multiple local optima. So gradient descent may not find a global optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in logisitic Regression we use an other Loss function and will give us an optimization problem that is <b>convex</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/01-logistic-regression/convex.png\" width = \"500px\" ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Loss (error) function in LOGISTIC REGRESSION:\n",
    "\n",
    "$$ L(y_{pred},y) = - (y log(y_{pred}) + (1-y) log(1-y_{pred})) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function is applied to just a single training example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Cost function in LOGISTIC REGRESSIONG:\n",
    "\n",
    "$$J(W,b) = \\frac{1}{m} \\sum_{i=1}^{m} L(y_{pred},y)$$\n",
    "\n",
    "$$J(W,b) = - \\frac{1}{m} \\sum_{i=1}^{m} (y^{(i)} log(y_{pred}^{(i)}) + (1-y^{(i)}) log(1-y_{pred}^{(i)})) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function is the cost of your parameters. So in training the logistic regression model, we are going to try to find parameters W and b that <b>minimize the overall cost function J</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use <b>the gradient descent algorithm</b> to train or to learn the parameters W and b on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Want to find W, b that minimize the cost function J(W,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see on the picture, the cost function J is convex. It is one of the huge reasons why we use this particular cost function J for logistic regression.\n",
    "\n",
    "- We wante to find the value of w and b that corresponds to the minilmum of the cost function J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/01-logistic-regression/descent_gradient.png\" width = \"300px\" ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So to find a good value for the parameters we need to initialize w and b.\n",
    "\n",
    "Usually we initialize the values of 0 but random initialization also works.\n",
    "\n",
    "> Gradient descent does it starts at that initial point and hopefully converge to the global optimum/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/01-logistic-regression/descent_gradient2.png\" width = \"300px\" ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example let's say we have the J(w) function which looks like this on the following picture. We ignore b to have a one dimensional plot for making this example easier.\n",
    "\n",
    "> So gradient descent updates the parameter W with the following formula :\n",
    "\n",
    "$$ W = W - \\alpha \\frac{\\partial{J(w)}}{\\partial w} $$\n",
    "$$ \\alpha = Learning \\ rate$$\n",
    "$$ \\frac{\\partial{J(w)}}{\\partial w} \\ is \\ the \\ slope $$\n",
    "\n",
    "\n",
    "Example: \n",
    "- if dJ/dw > 0, so decreasing w\n",
    "- if dJ/dw < 0, so increasing w\n",
    "\n",
    "And so hopefully gradient descent will move you to the global minimum of the J(w) function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/01-logistic-regression/descent_gradient3.png\" width = \"300px\" ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Formula :\n",
    "$$ W = W - \\alpha \\frac{\\partial{J(W,b)}}{\\partial W} $$\n",
    "$$ b = b - \\alpha \\frac{\\partial{J(W,b)}}{\\partial b} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Example for m =1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we consider that the dimension m is equal to 1 (one example or input). If we have a dataset of Iris flowers, and we try to predict if the Iris is toxic (y=1) or non-toxic (y=0).\n",
    "\n",
    "So we will make a binary classification with an input X which have n=2 features vectors x1 and x2, and the output y. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/01-logistic-regression/example_gradient_descent.PNG\" width = \"600px\" ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the logistic Regression algorithm for our model.\n",
    "\n",
    "- a represents y_pred or the activation\n",
    "- In our case J(w,b) = L(a,y) because we have m = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Dimension m = 1 / n = 2 - Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ z = w_1 x_1 + w_2 x_2 + b $$\n",
    "\n",
    "\n",
    "$$ a = \\sigma(z) = \\frac{1}{1+\\exp^{-z}}  $$\n",
    "\n",
    "$$ J(w,b) = L(a,y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model, we need to apply the descent gradient to update the parameters w1,w2 and b.\n",
    "\n",
    "$$ w_1 = w_1 - \\alpha \\frac{\\partial{J(w,b)}}{\\partial w_1} $$\n",
    "$$ w_2 = w_2 - \\alpha \\frac{\\partial{J(w,b)}}{\\partial w_2} $$\n",
    "$$ b = b - \\alpha \\frac{\\partial{J(w,b)}}{\\partial b} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We need to calculate :\n",
    "\n",
    "$$ \\frac{\\partial{J(w,b)}}{\\partial w_1} $$\n",
    "$$ \\frac{\\partial{J(w,b)}}{\\partial w_2} $$\n",
    "$$ \\frac{\\partial{J(w,b)}}{\\partial b} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Calcul for w1:\n",
    " \n",
    "$$ \\frac{\\partial{J}}{\\partial w_1} =  \\frac{\\partial{J}}{\\partial a}  \\frac{\\partial{a}}{\\partial z} \\frac{\\partial{z}}{\\partial w_1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/01-logistic-regression/calcul_w1.PNG\" width = \"600px\" ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Same method for w2 and b:\n",
    "$$ \\frac{\\partial{J}}{\\partial w_2} =  \\frac{\\partial{J}}{\\partial a}  \\frac{\\partial{a}}{\\partial z} \\frac{\\partial{z}}{\\partial w_2}$$\n",
    "$$ \\frac{\\partial{J}}{\\partial b} =  \\frac{\\partial{J}}{\\partial a}  \\frac{\\partial{a}}{\\partial z} \\frac{\\partial{z}}{\\partial b}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We obtain the following results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial{J}}{\\partial w_1} = (a-y) x_1$$\n",
    "\n",
    "$$ \\frac{\\partial{J}}{\\partial w_2} = (a-y) x_2$$\n",
    "\n",
    "$$ \\frac{\\partial{J}}{\\partial b} = (a-y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) m examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With m examples (and n = 2 features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(W,b) = - \\frac{1}{m} \\sum_{i=1}^{m} (y^{(i)} log(y_{pred}^{(i)}) + (1-y^{(i)}) log(1-y_{pred}^{(i)})) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We obtain the following results with m examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial{J}}{\\partial w_1} = - \\frac{1}{m} \\sum_{i=1}^{m} (a^{(i)}-y^{(i)})) x_1$$\n",
    "\n",
    "$$ \\frac{\\partial{J}}{\\partial w_2} =  - \\frac{1}{m} \\sum_{i=1}^{m} (a^{(i)}-y^{(i)}) x_2$$\n",
    "\n",
    "$$ \\frac{\\partial{J}}{\\partial b} =  - \\frac{1}{m} \\sum_{i=1}^{m} (a^{(i)}-y^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Dimension : m examples and n features by example\n",
    "\n",
    "$$X=\\begin{bmatrix} x^{(1)}_1 & x^{(2)}_1 .. & .. & x^{(m)}_1 \\\\ x^{(1)}_2 & x^{(2)}_2 .. & .. & x^{(m)}_2 \\\\. & . & . & .\\\\. & . & . & . \\\\ x^{(1)}_n & x^{(2)}_n .. & .. & x^{(m)}_n\\end{bmatrix} = \\begin{bmatrix}  X^{(1)} &  X^{(2)} & .. & .. &  X^{(m)} \\end{bmatrix}  \\in  \\mathbb{R^{n \\times m}}$$  \n",
    "\n",
    "\n",
    "$$W=\\begin{bmatrix} w_1 \\\\ w_2 \\\\ .. \\\\ .. \\\\ .. \\\\ w_n \\end{bmatrix} \\in  \\mathbb{R^{n \\times 1}}$$  \n",
    "\n",
    "$$y =\\begin{bmatrix} y_1 & y_2 & .. & .. & y_n\\end{bmatrix} \\in  \\mathbb{R^{1 \\times m}}$$ \n",
    "\n",
    "$$ b \\in  \\mathbb{R^{n \\times 1}}$$  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Model Vectorization :\n",
    "\n",
    "$$ \n",
    "    \\begin{cases}\n",
    "    Z = W^T X + b \\\\\n",
    "    A = \\sigma (Z) = \\frac{1}{1+\\exp^{-Z}}\n",
    "    \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Cost Function Vectorized:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$ J = - \\frac{1}{m} \\sum y \\times \\log(A) + (1-y) \\times \\log(1-A)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Gradient Descent Vectorized :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "    \\begin{cases}\n",
    "    W = W - \\alpha \\frac{\\partial{J}}{\\partial{W}}\\\\\n",
    "    b = b - \\alpha \\frac{\\partial{J}}{\\partial{b}} \\\\\n",
    "    \\frac{\\partial{J}}{\\partial{W}} = \\frac{1}{m} X (A-Y)^T\\\\\n",
    "    \\frac{\\partial{J}}{\\partial{b}} = \\frac{1}{m} \\sum (A-Y)\n",
    "    \\end{cases}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
