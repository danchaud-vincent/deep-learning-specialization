{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters Tuning and Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [1.Hyperparameter Tuning](#chapter1)\n",
    "    * [1.1 Tuning Process](#section_1_1)\n",
    "    * [1.2 Using an appropriate Scale to pick an Hyperparameters](#section_1_2)\n",
    "    * [1.3  ](#section_1_3)\n",
    "* [2. Batch Normalization](#chapter2)\n",
    "    * [2.1 ](#section_2_1)\n",
    "* [3. MultiClass Classification](#chapter3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Hyperparameter Tuning <a class=\"anchor\" id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Tuning Process <a class=\"anchor\" id=\"section_1_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a neural Network can involve setting a lot of hyperparameters. How to organize the hyperparameter tuning process to converge on the good settings of the hyperparameters.\n",
    "\n",
    "<u>Hyperparamters :</u>\n",
    "\n",
    "- $\\alpha$ : learning rate\n",
    "- $\\beta$ : momentum\n",
    "- $\\beta1$,$\\beta2$,$\\epsilon$ : Adam parameters\n",
    "- Number of layers\n",
    "- Number of hidden units\n",
    "- learning rate decay\n",
    "- mini-batch size\n",
    "\n",
    "\n",
    "<u>Most important Hyperparameters to tune :</u>\n",
    "\n",
    "- <p style=\"color:red;\">alpha</p>\n",
    "- <p style=\"color:orange;\">Beta</p>\n",
    "- <p style=\"color:orange;\">Number of hidden units</p>\n",
    "- <p style=\"color:orange;\">Mini-batch size</p>\n",
    "- <p style=\"color:purple;\">Number of layers</p>\n",
    "- <p style=\"color:purple;\">learning rate decay</p>\n",
    "- $\\beta1$,$\\beta2$,$\\epsilon$ : Almost always use $\\beta1$ = 0.9, $\\beta2$ = 0.999 and $\\epsilon = 10^{-8}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Tuning process :***\n",
    "\n",
    "- Try <b>random values of Hyperparameters</b> :\n",
    "    - <u>Example :</u> random values of hyper_param1 = alpha and hyper_param2 = epsilon\n",
    "\n",
    "- a <b>coarse to fine search process</b> :\n",
    "    - Try random values of hyperparameters\n",
    "    - select the ones that work the best\n",
    "    - Fine the search process and focus more resources on searching within this best values if you're suspecting that the best setting, the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Using an appropriate Scale to pick an Hyperparameters<a class=\"anchor\" id=\"section_1_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to pick the appropriate scale on which to explore the hyperparamaters.\n",
    "\n",
    "\n",
    "<u>Picking hyperparameters at random :</u>\n",
    "\n",
    "- Trying to choose the number of hidden units, $n^{[l]}$:\n",
    "    - Example: n[l] between 50,....,10\n",
    "\n",
    "- Trying to choose the number of layers, L:\n",
    "    - Example: L between 2,3 and 4\n",
    "\n",
    "Then sampling uniformly at random, migth be reasonable.\n",
    "\n",
    "\n",
    "<u>Appropriate Scale for hyperparameters :</u>\n",
    "\n",
    "- searching for $\\alpha$: \n",
    "    - values between 0.0001, .... ,1 \n",
    "\n",
    "If we sample values uniformly at random, then about 90% of the values you sample would be between 0.1 and 1. And only 10% of the resources to search between 0.0001 and 0.1. <br>\n",
    "<b>Instead, it seems more reasonable to search alpha on a log-scale than a linear scale.</b>\n",
    "\n",
    "--> search alpha in the scale : 0.0001 | ... | 0.001 | ... | 0.01 | ... | 0.1 | ... | 1\n",
    "\n",
    "To implement this we use :\n",
    "\n",
    "- r = -4 * np.random.randn() : $r \\in [-4,0]$\n",
    "- $\\alpha = 10^{r}$\n",
    "\n",
    "\n",
    "<u>Hyperparameters for exponentially weighted averages :</u>\n",
    "\n",
    "- searching  for $\\beta$ between 0.9, ..... , 0.999\n",
    "\n",
    "We need again to sample in a log-scale and not in a linear scale. Therefor we use $1-\\beta$ :\n",
    "\n",
    "--> search $1-\\beta$ in the scale : 0.001 | ... | 0.01 | ... | 0.1\n",
    "\n",
    "- r = -3 * np.random.randn() : $r \\in [-3,0]$\n",
    "- $1-\\beta = 10^{r}$\n",
    "- $\\beta = 1 - 10^{r}$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Batch Normalization <a class=\"anchor\" id=\"chapter2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Normalizing Activations in a Network <a class=\"anchor\" id=\"section_2_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Normalization makes your hyperparameters search problem much easier, make your neural network much more robust. It also enable us to much more easily train even very deep networks. \n",
    "\n",
    "\n",
    "**Normalize inputs to speed up learning :**<br>\n",
    "\n",
    "<u>Example with Logistic Regression</u>\n",
    "\n",
    "- $\\mu = \\frac{1}{m}\\sum_{i} X^{(i)}$\n",
    "- $ \\sigma =\\sqrt{\\frac{1}{m}\\sum_{i} X^{(i)^2}}$\n",
    "- $X_{norm} = \\frac{X-\\mu}{\\sigma}$\n",
    "\n",
    "<u>Deep neural Network :</u>\n",
    "\n",
    "- Normalize each layer $z^{[l]}$ to train much faster $W^{[l]}$ and $b^{[l]}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. MultiClass Classification <a class=\"anchor\" id=\"chapter3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
