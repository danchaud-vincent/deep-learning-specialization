{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters Tuning and Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [1.Hyperparameter Tuning](#chapter1)\n",
    "    * [1.1 Tuning Process](#section_1_1)\n",
    "    * [1.2 Using an appropriate Scale to pick an Hyperparameters](#section_1_2)\n",
    "* [2. Batch Normalization](#chapter2)\n",
    "    * [2.1 Normalizing Activations in a Network](#section_2_1)\n",
    "    * [2.2 Fitting batch norm into a neural network](#section_2_2)\n",
    "* [3. MultiClass Classification](#chapter3)\n",
    "    * [3.1 Softmax Regression](#section_3_1)\n",
    "    * [3.2 Training Softmax Classifier](#section_3_2)\n",
    "* [4. Recap](#chapter4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Hyperparameter Tuning <a class=\"anchor\" id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Tuning Process <a class=\"anchor\" id=\"section_1_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a neural Network can involve setting a lot of hyperparameters. How to organize the hyperparameter tuning process to converge on the good settings of the hyperparameters.\n",
    "\n",
    "<u>Hyperparamters :</u>\n",
    "\n",
    "- $\\alpha$ : learning rate\n",
    "- $\\beta$ : momentum\n",
    "- $\\beta1$,$\\beta2$,$\\epsilon$ : Adam parameters\n",
    "- Number of layers\n",
    "- Number of hidden units\n",
    "- learning rate decay\n",
    "- mini-batch size\n",
    "\n",
    "\n",
    "<u>Most important Hyperparameters to tune :</u>\n",
    "\n",
    "- <p style=\"color:red;\">alpha</p>\n",
    "- <p style=\"color:orange;\">Beta</p>\n",
    "- <p style=\"color:orange;\">Number of hidden units</p>\n",
    "- <p style=\"color:orange;\">Mini-batch size</p>\n",
    "- <p style=\"color:purple;\">Number of layers</p>\n",
    "- <p style=\"color:purple;\">learning rate decay</p>\n",
    "- $\\beta1$,$\\beta2$,$\\epsilon$ : Almost always use $\\beta1$ = 0.9, $\\beta2$ = 0.999 and $\\epsilon = 10^{-8}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Tuning process :***\n",
    "\n",
    "- Try <b>random values of Hyperparameters</b> :\n",
    "    - <u>Example :</u> random values of hyper_param1 = alpha and hyper_param2 = epsilon\n",
    "\n",
    "- a <b>coarse to fine search process</b> :\n",
    "    - Try random values of hyperparameters\n",
    "    - select the ones that work the best\n",
    "    - Fine the search process and focus more resources on searching within this best values if you're suspecting that the best setting, the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Using an appropriate Scale to pick an Hyperparameters<a class=\"anchor\" id=\"section_1_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to pick the appropriate scale on which to explore the hyperparamaters.\n",
    "\n",
    "\n",
    "<u>Picking hyperparameters at random :</u>\n",
    "\n",
    "- Trying to choose the number of hidden units, $n^{[l]}$:\n",
    "    - Example: n[l] between 50,....,10\n",
    "\n",
    "- Trying to choose the number of layers, L:\n",
    "    - Example: L between 2,3 and 4\n",
    "\n",
    "Then sampling uniformly at random, migth be reasonable.\n",
    "\n",
    "\n",
    "<u>Appropriate Scale for hyperparameters :</u>\n",
    "\n",
    "- searching for $\\alpha$: \n",
    "    - values between 0.0001, .... ,1 \n",
    "\n",
    "If we sample values uniformly at random, then about 90% of the values you sample would be between 0.1 and 1. And only 10% of the resources to search between 0.0001 and 0.1. <br>\n",
    "<b>Instead, it seems more reasonable to search alpha on a log-scale than a linear scale.</b>\n",
    "\n",
    "--> search alpha in the scale : 0.0001 | ... | 0.001 | ... | 0.01 | ... | 0.1 | ... | 1\n",
    "\n",
    "To implement this we use :\n",
    "\n",
    "- r = -4 * np.random.randn() : $r \\in [-4,0]$\n",
    "- $\\alpha = 10^{r}$\n",
    "\n",
    "\n",
    "<u>Hyperparameters for exponentially weighted averages :</u>\n",
    "\n",
    "- searching  for $\\beta$ between 0.9, ..... , 0.999\n",
    "\n",
    "We need again to sample in a log-scale and not in a linear scale. Therefor we use $1-\\beta$ :\n",
    "\n",
    "--> search $1-\\beta$ in the scale : 0.001 | ... | 0.01 | ... | 0.1\n",
    "\n",
    "- r = -3 * np.random.randn() : $r \\in [-3,0]$\n",
    "- $1-\\beta = 10^{r}$\n",
    "- $\\beta = 1 - 10^{r}$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Batch Normalization <a class=\"anchor\" id=\"chapter2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Normalizing Activations in a Network <a class=\"anchor\" id=\"section_2_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Normalization makes your hyperparameters search problem much easier, make your neural network much more robust. It also enable us to much more easily train even very deep networks. \n",
    "\n",
    "\n",
    "**Normalize inputs to speed up learning :**<br>\n",
    "\n",
    "<u>Example with Logistic Regression</u>\n",
    "\n",
    "- $\\mu = \\frac{1}{m}\\sum_{i} X^{(i)}$\n",
    "- $ \\sigma =\\sqrt{\\frac{1}{m}\\sum_{i} X^{(i)^2}}$\n",
    "- $X_{norm} = \\frac{X-\\mu}{\\sigma}$\n",
    "\n",
    "<u>Deep neural Network :</u>\n",
    "\n",
    "- Normalize each layer $z^{[l]}$ to train much faster $W^{[l]}$ and $b^{[l]}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Fitting batch norm into a neural network <a class=\"anchor\" id=\"section_2_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. MultiClass Classification <a class=\"anchor\" id=\"chapter3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Softmax Regression <a class=\"anchor\" id=\"section_3_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, the classification example we talked about have used binary classification, where you had two possible labels, 0 or 1.\n",
    "\n",
    "***What if we have multiple possible classes?***\n",
    "\n",
    "There is a generalization of logistic regression called <b>Softmax Regression</b> that make predictions when you are trying to recognize one of multiple classes, rather than just recognize two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Example :</u>\n",
    "\n",
    "- We are trying to recognize cats, dogs, baby chicks and koalas:\n",
    "    - koala_class = 0\n",
    "    - cat_class = 1\n",
    "    - dog_class = 2\n",
    "    - chick_class = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So We're going to build a neural network where the output layer has 4 output units.\n",
    "\n",
    "\n",
    "<center><img src=\"images/09-hyperparameters tuning/softmax.PNG\" width =\"500px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wante the number of units in the output layer to tell us what is the probability of each of these four classes.\n",
    "\n",
    "- 1st node : output the probability of as a kaola given X\n",
    "- 2nd node : output the probability of as a cat given X\n",
    "- 3rd node : output the probability of as a dog given X\n",
    "- 4th node : output the probability of as a chick given X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Softmax Layer :**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/09-hyperparameters tuning/softmax2.PNG\" width =\"500px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the L layer:\n",
    "\n",
    "- $Z^{[L]} = W^{[L]} a^{[L-1]} + b^{[L]}$\n",
    "- Activation Function :\n",
    "    - $t = e^{Z^{[L]}}$\n",
    "    - $a^{[L]} = \\frac{e^{Z^{[L]}}}{\\sum_{i=1}^{4}t_i}$\n",
    "\n",
    "<u>In our Example with 4 classes :</u>\n",
    "- $t \\in (4 \\times 1)$\n",
    "- $a^{[L]} \\in (4 \\times 1)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Example with values :</u>\n",
    "\n",
    "- $ Z^{[L]} = \\begin{bmatrix} 5 \\\\ 2 \\\\ -1 \\\\ 3 \\end{bmatrix}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $ t = \\begin{bmatrix} e^5 \\\\ e^2 \\\\ e^{-1} \\\\ e^3 \\end{bmatrix} = \\begin{bmatrix} 148.4 \\\\ 7.4 \\\\ 0.4 \\\\ 20.1 \\end{bmatrix}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- $\\sum_{i=1}^{4}t_i = 176.3 $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation :\n",
    "- $ y_{pred} = a^{[L]} = \\frac{t}{176.3} = \\begin{bmatrix} 0.842 \\\\ 0.042 \\\\ 0.002 \\\\ 0.114 \\end{bmatrix}$ \n",
    "\n",
    "res = 0.842 + 0.042 + 0.002 + 0.114 = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/09-hyperparameters tuning/example.PNG\" width =\"300px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Training Softmax Classifier <a class=\"anchor\" id=\"section_3_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss function :**\n",
    "\n",
    "- C : number of classes\n",
    "\n",
    "$L(y_{pred},y) = - \\sum_{j=1}^{C} y_j*log(y_{j_{pred}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cost Function on m examples :**\n",
    "\n",
    "$J(W^{[1]},b^{[1]},...,W^{[L]},b^{[L]}) = \\frac{1}{m} \\sum_{i=1}^{m} L(y_{pred}^{(i)},y^{(i)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Recap <a class=\"anchor\" id=\"chapter4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. If searching among a large number of hyperparameters, you should try random values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Some hyperparameters, such as the learning rate, are more critical than others. So a hyperparameter can have a huge negative impact on training if it set poorly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. If $\\beta$ (hyperparameter for momentum) is between 0.9 and 0.99, then a way to sample a value for beta is :\n",
    "    - r = np.random.randn()\n",
    "    - beta = 1-10**(-r-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. In batch normalization, if we applied it on the lth layer, then the normalization is applied on $z^{[l]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. In the normalization formula \n",
    "\n",
    "    - $z_{norm}^{(i)} = \\frac{z^{(i)} - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$\n",
    "    - $\\epsilon$ avoid division by zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. $\\gamma$ and $\\beta$ in Batch Norm :\n",
    "\n",
    "    - set the mean and variance of the linear variable $z^{[l]}$ of a given layer\n",
    "    - They can be learned using Adam, Gradient Descent, Momentum or RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. After training a neural network with Batch Norm, at test time, to evaluate the neural network on a new example we should:\n",
    "\n",
    "    - Perform the needed normalizations, use $\\mu$ and $\\sigma^2$ estimated using an exponentially weighted average across mini-batches seen during training.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
