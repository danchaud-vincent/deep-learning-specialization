{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization Algorithms enable us to train our neural network much faster. \n",
    "\n",
    "Machine Learning is a Highly empirical process. We train a lot of models in order to find one that works really well. So it really helps to train our models quickly. <br>\n",
    "In deep learning we train our models with large dataset, and so the training can be really slow. So having good optimization algorithms can really speed up the efficiency of you and your team."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [1.  Mini-batch Gradient](#chapter1)\n",
    "    * [1.1 Batch vs Mini-batch Gradient Descent](#section_1_1)\n",
    "    * [1.2 Understanding Mini-batch Gradient Descent](#section_1_2)\n",
    "    * [1.3 Gradient Descent with Momentum](#section_1_3)\n",
    "    * [1.4 RMSpop](#section_1_4)\n",
    "    * [1.5 Adam Optimization Algorithm](#section_1_5)\n",
    "    * [1.6 Learning Rate Decay](#section_1_6)\n",
    "* [2. Recap](#chapter2)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Mini-batch Gradient <a class=\"anchor\" id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Batch vs Mini-batch Gradient Descent <a class=\"anchor\" id=\"section_1_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization allows us to compute on all m examples. We saw that we can vectorize all our examples in Matrix:\n",
    "\n",
    "$$ X= \\begin{bmatrix} X^{(1)} & X^{(2)} & .. & .. & .. & X^{(m)}\\end{bmatrix} \\in \\mathbb{(n_x \\times m)}$$\n",
    "$$ y= \\begin{bmatrix} y^{(1)} & y^{(2)} & .. & .. & .. & y^{(m)}\\end{bmatrix} \\in \\mathbb{(1 \\times m)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization allows you to process all M examples relatively quickly. But if M is very large then it can still be slow.<br>\n",
    "With the implementation of gradient descent on your whole training set, what you have to do is, you have to process your entire training set before you take another little step of gradient descent.\n",
    "\n",
    "> So it turns out that you can get a faster algorithm if you let gradient descent start to make some progress even before you finish processing the entire big training sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini-batch Example :\n",
    "\n",
    "- m examples = 10000\n",
    "- mini-batch sizes = 1000 (1000 example in each mini-batch)\n",
    "\n",
    "$$ X= \\begin{bmatrix} X^{(1)} & X^{(2)} & .. & X^{(1000)} |X^{(1001)} & .. & X^{(2000)}|... & ... & X^{(m)}\\end{bmatrix} $$ \n",
    "$$ y= \\begin{bmatrix} y^{(1)} & y^{(2)} & .. & y^{(1000)} | .. & .. & .. & .. | .. & .. & y^{(m)}\\end{bmatrix} $$\n",
    "\n",
    "> Mini batch 1 :\n",
    "$$ X ^{\\{1\\}}= \\begin{bmatrix} X^{(1)} & X^{(2)} & .. & X^{(1000)}\\end{bmatrix} \\in (n_x \\times 1000)$$\n",
    "$$ y ^{\\{1\\}} = \\begin{bmatrix} y^{(1)} & y^{(2)} & .. & y^{(1000)}\\end{bmatrix} \\in (1 \\times 1000) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we are processing the training process on the Mini-batch 1:\n",
    "\n",
    "- Forward propagation on mini-batch 1\n",
    "- Compute the Loss $J^{\\{1\\}}$ according to the mini-batch 1\n",
    "- Backpropagation to compute gradients with respect to $J^{\\{1\\}}$\n",
    "\n",
    "We repeat these steps on all the mini-batchs.\n",
    "\n",
    "For t in number_of_mini_batchs:\n",
    "- Forward propagation on mini-batch t\n",
    "- Compute the Loss $J^{\\{t\\}}$ according to the mini-batch 1\n",
    "- Backpropagation to compute gradients with respect to $J^{\\{t\\}}$\n",
    "\n",
    "\n",
    "By doing these steps on all the mini-batches we have done 1 <b>epoch</b>\n",
    "\n",
    "> epoch is a word that means a single pass through the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Understanding Mini-batch Gradient Descent <a class=\"anchor\" id=\"section_1_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/08-Optimization algorithms/mini-batch.PNG\" width = \"600px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With batch gradient descent on every iteration you go through the entire training set and you'd expect the cost to go down on every single iteration.\n",
    "\n",
    "- With mini batch gradient descent: if you plot the cost function $J^{\\{t\\}}$, it should trend downwards, but it's also going to be a little bit noisier. The reason is maybe  $X^{\\{1\\}},y^{\\{1\\}}$ is the rows of easy minibatch so the cost will be lower, and after $X^{\\{2\\}},y^{\\{2\\}}$ will be a harder mini-batch and so your cost will be higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Choising the mini-batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If mini-batch size = m : Batch gradient descent\n",
    "    - your mini-batch is all the example\n",
    "- If mini-batch size = 1 : stochastic gradient descent\n",
    "    - your mini-batch is one single example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Representation cost function</u>:\n",
    "\n",
    "- <b>Batch gradient descent</b> might start somewhere and be able to take relatively low noise, relatively large steps. And you could just keep matching to the minimum. \n",
    "- <b>Stochastic gradient descent</b>: on every iteration you're taking gradient descent with just a single strain example. So sometimes you it will head to the good direction and sometimes it will hit in the wrong direction. So stochastic gradient descent can be extremely noisy. <i><b>And stochastic gradient descent won't ever converge</b><i/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/08-Optimization algorithms/batchy-size.PNG\" width = \"300px\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Choosing mini-batch size</u>:\n",
    "\n",
    "<center>\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Stochastic Gradient descent</th>\n",
    "            <th>Mini-batch Gradient descent</th>\n",
    "            <th>Batch Gradient descent</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>Loose speedup from vectorization</td>\n",
    "            <td>Fastest learning</td>\n",
    "            <td>Too long per iteration</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If small training set : (m<2000):\n",
    "    - Batch gradient descent\n",
    "\n",
    "- Else <b>Typical mini-batch size</b>:\n",
    "    - 64\n",
    "    - 128\n",
    "    - 256\n",
    "    - 512\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the mini-batch is another hyperparameters of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Gradient Descent with Momentum <a class=\"anchor\" id=\"section_1_3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an algorithm called Gradient Descent with Momentum that almost always works faster than the standard gradient descent. The basic idea is to compute an exponentially weighted average of your gradients, and then use that gradient to update your weights instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The idea of the algorithm:\n",
    "- init $ V_{dW}$ and $ V_{db}$ :\n",
    "    - $ V_{dW}$ = 0\n",
    "    - $ V_{db}$ =0\n",
    "\n",
    "- Compute dW and db\n",
    "- Computing Exponentially moving average :\n",
    "\n",
    "$$ V_{dW} = \\beta  \\ \\  V_{dW}+ (1-\\beta)dW $$\n",
    "$$ V_{db} = \\beta \\ \\ V_{db} + (1-\\beta)db$$\n",
    "\n",
    "- Update the parameters :\n",
    "\n",
    "$$ W = W - \\alpha *  V_{dW}$$\n",
    "$$ b = b - \\alpha * V_{db}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\beta$ is a hyperparameter which control exponentially moving average. A good value of $\\beta$ is 0.9\n",
    "\n",
    "> The Gradient Descent with momentum is a technique use to speed up the learning algorithm, and have a gradient descent which converge faster to the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 RMSpop <a class=\"anchor\" id=\"section_1_4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> RMSpop : Root Mean Squared pop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSpop is an other way to optimize the gradient Descent and speed up the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The idea of the algorithm:\n",
    "\n",
    "- init $ S_{dW}$ and $ S_{db}$ :\n",
    "    - $ S_{dW}$ = 0\n",
    "    - $ S_{db}$ =0\n",
    "\n",
    "- Compute dW and db\n",
    "- Computing RMSpop :\n",
    "\n",
    "$$ S_{dW} = \\beta_2  \\ \\  S_{dW}+ (1-\\beta_2)dW^2 $$\n",
    "$$ S_{db} = \\beta_2 \\ \\ S_{db} + (1-\\beta_2)db^2$$\n",
    "\n",
    "- Update the parameters :\n",
    "\n",
    "$$ W = W - \\alpha *  \\frac{dW}{\\sqrt{S_{dW}} + \\epsilon }$$\n",
    "$$ b = b - \\alpha * \\frac{db}{\\sqrt{S_{db}} +  \\epsilon} $$\n",
    "$$\\epsilon ~= 10^{-8}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's RMSprop, and similar to momentum, has the effects of damping out the oscillations in gradient descent, in mini-batch gradient descent. And allowing you to maybe use a larger learning rate alpha. And certainly speeding up the learning speed of your algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that if you put them together you can get an even better optimization algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Adam Optimization Algorithm <a class=\"anchor\" id=\"section_1_5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Adam : Adaptive moment estimation\n",
    "\n",
    "The Adam optimization algorithm is basically taking momentum and RMSprop, and putting them together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Adam algorithm optimization:\n",
    "\n",
    "- init  $S_{dW}$ and $ S_{db}$ and $V_{dW}$ and $ V_{db}$:\n",
    "    - $V_{dW}$ = 0\n",
    "    - $V_{db}$ = 0\n",
    "    - $S_{dW}$ = 0\n",
    "    - $S_{db}$ = 0\n",
    "\n",
    "- Compute dW and db\n",
    "- Computing Adam algorithm:\n",
    "\n",
    "    - On Iteration t:\n",
    "\n",
    "$$ \n",
    "\\begin{cases}\n",
    "    V_{dW} = \\beta_1  \\ \\  V_{dW}+ (1-\\beta_1)dW \\\\\n",
    "    V_{db} = \\beta_1 \\ \\ V_{db} + (1-\\beta_1)db\n",
    "\\end{cases}\n",
    "$$\n",
    "$$ \n",
    "\\begin{cases}\n",
    "    S_{dW} = \\beta_2  \\ \\  S_{dW}+ (1-\\beta_2)dW^2 \\\\\n",
    "    S_{db} = \\beta_2 \\ \\ S_{db} + (1-\\beta_2)db^2\n",
    "\\end{cases}\n",
    "$$\n",
    "$$ \n",
    "\\begin{cases}\n",
    "    V_{dW}^{corrected} = \\frac{V_{dW}}{1-\\beta_1^t} \\\\\n",
    "    V_{db}^{corrected} = \\frac{V_{db}}{1-\\beta_1^t}\n",
    "\\end{cases}\n",
    "$$\n",
    "$$ \n",
    "\\begin{cases}\n",
    "    S_{dW}^{corrected} = \\frac{S_{dW}}{1-\\beta_2^t} \\\\\n",
    "    S_{db}^{corrected} = \\frac{S_{db}}{1-\\beta_2^t}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "- Update the parameters :\n",
    "\n",
    "$$ W = W - \\alpha *  \\frac{V_{dW}^{corrected}}{\\sqrt{S_{dW}^{corrected}}+  \\epsilon }$$\n",
    "$$ b = b - \\alpha * \\frac{ V_{db}^{corrected}}{\\sqrt{S_{db}^{corrected}}+  \\epsilon} $$\n",
    "$$\\epsilon ~= 10^{-8}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Hyperparameters choices:\n",
    "\n",
    "- $\\alpha$ = needs to be tune\n",
    "- $\\beta_1$ = 0.9 (dW)\n",
    "- $\\beta_2$ = 0.999 ($dW^2$)\n",
    "- $\\epsilon = 10^{-8}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Learning Rate Decay <a class=\"anchor\" id=\"section_1_6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the things that might help speed up your learning algorithm is to slowly reduce your learning rate over time. We call this learning rate decay. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning Rate Decay methods decrease the learning_rate gradually at each epochs. The goal is to optimize the learning process of our model at each epochs. \n",
    "\n",
    "For examplein the beginning, the gradient descent might be noisy and tend towards the minimum, but it won't exactly converge because we use a fixed value of alpha(learning_rate). The intuition behind slowly reducing Alpha is that maybe during the initial steps of learning, you could afford to take much bigger steps, but then as learning approaches convergence, then having a slower learning rate allows you to take smaller steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Example of learning rate decay</u>:\n",
    "\n",
    "$$ \\alpha = \\frac{1}{1+ decayRate * epoch_{num}} * \\alpha_0$$\n",
    "\n",
    "$$ \\alpha = 0.95^{epoch_{num}}*\\alpha_0$$\n",
    "\n",
    "$$ \\alpha = \\frac{k}{\\sqrt{epoch_{num}}}*\\alpha_0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Recap <a class=\"anchor\" id=\"chapter2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Mini-Batch:\n",
    "\n",
    "- If the mini-batch size is m, you end up with batch gradient descent, which has to process the whole training set before making progress. \n",
    "- If the mini-batch size is 1, you lose the benefits of vectorization across examples in the mini-batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Optimization Algorithm : \n",
    "\n",
    "<center><img src=\"images/08-Optimization algorithms/optimization_process.PNG\" width = \"500px\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (1) : Gradient descent\n",
    "- (2) : Gradient descent with momentum ($\\beta $= 0.5)\n",
    "- (3) : Gradient descent with momentum ($\\beta $= 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Adam :\n",
    "\n",
    "- Adam combines the advantages of RMSProp and momentum\n",
    "- The learning rate hyperparameter \\alphaα in Adam usually needs to be tuned.\n",
    "- We usually use “default” values for the hyperparameters $\\beta_1$, $\\beta_2$, $\\epsilon$ :\n",
    "    - $\\beta_1 = 0.9$\n",
    "    - $\\beta_2 = 0.999$\n",
    "    - $\\epsilon = 10^{-8}$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
