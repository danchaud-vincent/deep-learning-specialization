{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization Algorithms enable us to train our neural network much faster. \n",
    "\n",
    "Machine Learning is a Highly empirical process. We train a lot of models in order to find one that works really well. So it really helps to train our models quickly. <br>\n",
    "In deep learning we train our models with large dataset, and so the training can be really slow. So having good optimization algorithms can really speed up the efficiency of you and your team."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [1.  Mini-batch Gradient](#chapter1)\n",
    "    * [1.1 Batch vs Mini-batch Gradient Descent](#section_1_1)\n",
    "    * [1.2 Understanding Mini-batch Gradient Descent](#section_1_2)\n",
    "    \n",
    "* [2. ](#chapter2)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Mini-batch Gradient <a class=\"anchor\" id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Batch vs Mini-batch Gradient Descent <a class=\"anchor\" id=\"section_1_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization allows us to compute on all m examples. We saw that we can vectorize all our examples in Matrix:\n",
    "\n",
    "$$ X= \\begin{bmatrix} X^{(1)} & X^{(2)} & .. & .. & .. & X^{(m)}\\end{bmatrix} \\in \\mathbb{(n_x \\times m)}$$\n",
    "$$ y= \\begin{bmatrix} y^{(1)} & y^{(2)} & .. & .. & .. & y^{(m)}\\end{bmatrix} \\in \\mathbb{(1 \\times m)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization allows you to process all M examples relatively quickly. But if M is very large then it can still be slow.<br>\n",
    "With the implementation of gradient descent on your whole training set, what you have to do is, you have to process your entire training set before you take another little step of gradient descent.\n",
    "\n",
    "> So it turns out that you can get a faster algorithm if you let gradient descent start to make some progress even before you finish processing the entire big training sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini-batch Example :\n",
    "\n",
    "- m examples = 10000\n",
    "- mini-batch sizes = 1000 (1000 example in each mini-batch)\n",
    "\n",
    "$$ X= \\begin{bmatrix} X^{(1)} & X^{(2)} & .. & X^{(1000)} |X^{(1001)} & .. & X^{(2000)}|... & ... & X^{(m)}\\end{bmatrix} $$ \n",
    "$$ y= \\begin{bmatrix} y^{(1)} & y^{(2)} & .. & y^{(1000)} | .. & .. & .. & .. | .. & .. & y^{(m)}\\end{bmatrix} $$\n",
    "\n",
    "> Mini batch 1 :\n",
    "$$ X ^{\\{1\\}}= \\begin{bmatrix} X^{(1)} & X^{(2)} & .. & X^{(1000)}\\end{bmatrix} \\in (n_x \\times 1000)$$\n",
    "$$ y ^{\\{1\\}} = \\begin{bmatrix} y^{(1)} & y^{(2)} & .. & y^{(1000)}\\end{bmatrix} \\in (1 \\times 1000) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we are processing the training process on the Mini-batch 1:\n",
    "\n",
    "- Forward propagation on mini-batch 1\n",
    "- Compute the Loss $J^{\\{1\\}}$ according to the mini-batch 1\n",
    "- Backpropagation to compute gradients with respect to $J^{\\{1\\}}$\n",
    "\n",
    "We repeat these steps on all the mini-batchs.\n",
    "\n",
    "For t in number_of_mini_batchs:\n",
    "- Forward propagation on mini-batch t\n",
    "- Compute the Loss $J^{\\{t\\}}$ according to the mini-batch 1\n",
    "- Backpropagation to compute gradients with respect to $J^{\\{t\\}}$\n",
    "\n",
    "\n",
    "By doing these steps on all the mini-batches we have done 1 <b>epoch</b>\n",
    "\n",
    "> epoch is a word that means a single pass through the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Understanding Mini-batch Gradient Descent <a class=\"anchor\" id=\"section_1_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/08-Optimization algorithms/mini-batch.PNG\" width = \"600px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With batch gradient descent on every iteration you go through the entire training set and you'd expect the cost to go down on every single iteration.\n",
    "\n",
    "- With mini batch gradient descent: if you plot the cost function $J^{\\{t\\}}$, it should trend downwards, but it's also going to be a little bit noisier. The reason is maybe  $X^{\\{1\\}},y^{\\{1\\}}$ is the rows of easy minibatch so the cost will be lower, and after $X^{\\{2\\}},y^{\\{2\\}}$ will be a harder mini-batch and so your cost will be higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Choising the mini-batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If mini-batch size = m : Batch gradient descent\n",
    "    - your mini-batch is all the example\n",
    "- If mini-batch size = 1 : stochastic gradient descent\n",
    "    - your mini-batch is one single example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Representation cost function</u>:\n",
    "\n",
    "- <b>Batch gradient descent</b> might start somewhere and be able to take relatively low noise, relatively large steps. And you could just keep matching to the minimum. \n",
    "- <b>Stochastic gradient descent</b>: on every iteration you're taking gradient descent with just a single strain example. So sometimes you it will head to the good direction and sometimes it will hit in the wrong direction. So stochastic gradient descent can be extremely noisy. <i><b>And stochastic gradient descent won't ever converge</b><i/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/08-Optimization algorithms/batchy-size.PNG\" width = \"300px\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Choosing mini-batch size</u>:\n",
    "\n",
    "<center>\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Stochastic Gradient descent</th>\n",
    "            <th>Mini-batch Gradient descent</th>\n",
    "            <th>Batch Gradient descent</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>Loose speedup from vectorization</td>\n",
    "            <td>Fastest learning</td>\n",
    "            <td>Too long per iteration</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If small training set : (m<2000):\n",
    "    - Batch gradient descent\n",
    "\n",
    "- Else <b>Typical mini-batch size</b>:\n",
    "    - 64\n",
    "    - 128\n",
    "    - 256\n",
    "    - 512\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the mini-batch is another hyperparameters of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
