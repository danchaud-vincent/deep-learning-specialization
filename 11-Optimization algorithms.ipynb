{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization Algorithms enable us to train our neural network much faster. \n",
    "\n",
    "Machine Learning is a Highly empirical process. We train a lot of models in order to find one that works really well. So it really helps to train our models quickly. <br>\n",
    "In deep learning we train our models with large dataset, and so the training can be really slow. So having good optimization algorithms can really speed up the efficiency of you and your team."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [1.  Mini-batch Gradient](#chapter1)\n",
    "    * [1.1 Batch vs Mini-batch Gradient Descent](#section_1_1)\n",
    "* [2. ](#chapter2)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Mini-batch Gradient <a class=\"anchor\" id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Batch vs Mini-batch Gradient Descent <a class=\"anchor\" id=\"section_1_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization allows us to compute on all m examples. We saw that we can vectorize all our examples in Matrix:\n",
    "\n",
    "$$ X= \\begin{bmatrix} X^{(1)} & X^{(2)} & .. & .. & .. & X^{(m)}\\end{bmatrix} \\in \\mathbb{(n_x \\times m)}$$\n",
    "$$ y= \\begin{bmatrix} y^{(1)} & y^{(2)} & .. & .. & .. & y^{(m)}\\end{bmatrix} \\in \\mathbb{(1 \\times m)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization allows you to process all M examples relatively quickly. But if M is very large then it can still be slow.<br>\n",
    "With the implementation of gradient descent on your whole training set, what you have to do is, you have to process your entire training set before you take another little step of gradient descent.\n",
    "\n",
    "> So it turns out that you can get a faster algorithm if you let gradient descent start to make some progress even before you finish processing the entire big training sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini-batch Example :\n",
    "\n",
    "- m examples = 10000\n",
    "- mini-batch sizes = 1000 (1000 example in each mini-batch)\n",
    "\n",
    "$$ X= \\begin{bmatrix} X^{(1)} & X^{(2)} & .. & X^{(1000)} |X^{(1001)} & .. & X^{(2000)}|... & ... & X^{(m)}\\end{bmatrix} $$ \n",
    "$$ y= \\begin{bmatrix} y^{(1)} & y^{(2)} & .. & y^{(1000)} | .. & .. & .. & .. | .. & .. & y^{(m)}\\end{bmatrix} $$\n",
    "\n",
    "> Mini batch 1 :\n",
    "$$ X ^{\\{1\\}}= \\begin{bmatrix} X^{(1)} & X^{(2)} & .. & X^{(1000)}\\end{bmatrix} \\in (n_x \\times 1000)$$\n",
    "$$ y ^{\\{1\\}} = \\begin{bmatrix} y^{(1)} & y^{(2)} & .. & y^{(1000)}\\end{bmatrix} \\in (1 \\times 1000) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we are processing the training process on the Mini-batch 1:\n",
    "\n",
    "- Forward propagation on mini-batch 1\n",
    "- Compute the Loss $J^{\\{1\\}}$ according to the mini-batch 1\n",
    "- Backpropagation to compute gradients with respect to $J^{\\{1\\}}$\n",
    "\n",
    "We repeat these steps on all the mini-batchs.\n",
    "\n",
    "For t in number_of_mini_batchs:\n",
    "- Forward propagation on mini-batch t\n",
    "- Compute the Loss $J^{\\{t\\}}$ according to the mini-batch 1\n",
    "- Backpropagation to compute gradients with respect to $J^{\\{t\\}}$\n",
    "\n",
    "\n",
    "By doing these steps on all the mini-batches we have done 1 <b>epoch</b>\n",
    "\n",
    "> epoch is a word that means a single pass through the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
