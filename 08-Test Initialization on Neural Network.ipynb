{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Initialization on Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [1. Build a L-layers Neural Network ](#chapter1)\n",
    "    * [1.1 Initialize parameters](#section_1_1)\n",
    "    * [1.2 Forward propagation](#section_1_2)\n",
    "    * [1.3 Cost function](#section_1_3)\n",
    "    * [1.4 Backward Propagation](#section_1_4)\n",
    "    * [1.5 Update parameters](#section_1_5) \n",
    "    * [1.6 Predict](#section_1_6) \n",
    "    * [1.7 Model](#section_1_7)\n",
    "* [2. Example on Dataset 1](#chapter2)\n",
    "    * [2.1 Load the Dataset](#section_2_1)\n",
    "    * [2.2 Display the Data](#section_2_2)\n",
    "    * [2.3 Flatten the data](#section_2)\n",
    "    * [2.4 Normalize the data](#section_2_4)\n",
    "* [3. Example on Dataset 2](#chapter3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Build a L-layer Neural Network <a class=\"anchor\" id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 initialization parameters  <a class=\"anchor\" id=\"section_1_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layers_dims,n_input,n_output,type_init=\"random\"):\n",
    "    \"\"\"\n",
    "    Compute the initialization of the parameters in our Neural Network\n",
    "\n",
    "    - Arguments:\n",
    "    layers_dims: array containing the dimension of the hidden layers\n",
    "    n_input: numbers of features in the input layer\n",
    "    n_ouput: numbers of nodes in the output layer\n",
    "    type_init: \"zeros\",\"random\",\"he\" type of initialization\n",
    "\n",
    "    - Return:\n",
    "    parameters: dictionnary containing of the parameters of our neural network\n",
    "    \"\"\"\n",
    "\n",
    "    # init\n",
    "    parameters = {}\n",
    "\n",
    "    # add the output layer to the array\n",
    "    layers_dims.append(n_output)\n",
    "\n",
    "    # number of layers\n",
    "    L = len(layers_dims)\n",
    "\n",
    "    for i in range(L):\n",
    "        \n",
    "        # if i==0 take n_x features\n",
    "        if i ==0:\n",
    "            layer_prev = n_input\n",
    "        else:\n",
    "            layer_prev = layers_dims[i-1]\n",
    "\n",
    "\n",
    "        # check type of initialization\n",
    "        if type_init.lower() == \"random\":\n",
    "\n",
    "            parameters[\"W\" + str(i+1)] = np.random.randn(layers_dims[i],layer_prev) * 10\n",
    "            parameters[\"b\" + str(i+1)] = np.zeros((layers_dims[i],1))\n",
    "\n",
    "        elif type_init.lower() == \"zeros\":\n",
    "\n",
    "            parameters[\"W\" + str(i+1)] = np.zeros((layers_dims[i],layer_prev))\n",
    "            parameters[\"b\" + str(i+1)] = np.zeros((layers_dims[i],1))\n",
    "        \n",
    "        elif type_init.lower() == \"he\":\n",
    "\n",
    "            parameters[\"W\" + str(i+1)] = np.random.randn(layers_dims[i],layer_prev) * np.sqrt(2/layer_prev)\n",
    "            parameters[\"b\" + str(i+1)] = np.zeros((layers_dims[i],1))\n",
    "\n",
    "        else:\n",
    "            # default init\n",
    "            parameters[\"W\" + str(i+1)] = np.random.randn(layers_dims[i],layer_prev) * np.sqrt(2/layer_prev)\n",
    "            parameters[\"b\" + str(i+1)] = np.zeros((layers_dims[i],1))\n",
    "\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 11.32560366,  -3.26043072],\n",
       "        [ -2.11363361,  -4.14921609],\n",
       "        [ -4.08071049, -13.67506506],\n",
       "        [  9.58239619,  -9.16575335],\n",
       "        [  8.16774933,  -7.11651153]]),\n",
       " 'b1': array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'W2': array([[ 19.68693908,  18.34366201,  10.59021041, -13.00776998,\n",
       "          -0.25951835],\n",
       "        [ -4.13002775,  14.09642669,  -3.23203552,   7.88064143,\n",
       "         -12.33612276],\n",
       "        [  2.50541358,  -4.40450762,  -8.51534481,   5.31997459,\n",
       "          -7.0632816 ],\n",
       "        [  0.99583807,   3.5692665 ,  10.22561226,  17.05603498,\n",
       "          -2.97087116],\n",
       "        [  0.82053045,  -9.18743069,  -0.89199566, -22.23700812,\n",
       "           4.25948501]]),\n",
       " 'b2': array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'W3': array([[ -7.08581924,   8.69877027,  -0.59588335,  -3.4479965 ,\n",
       "           0.25808352],\n",
       "        [  5.13541146,  18.04046724,   7.58992139, -11.12741265,\n",
       "         -10.86649237],\n",
       "        [ 15.81986861,  18.90498362,  -2.90841281, -10.046818  ,\n",
       "          -2.82640126]]),\n",
       " 'b3': array([[0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'W4': array([[ 8.5874565 , -3.09750503, -0.72574527]]),\n",
       " 'b4': array([[0.]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test initialization\n",
    "hidden_layers_dim = [5,5,3]\n",
    "n_input = 2\n",
    "n_output = 1\n",
    "\n",
    "params = initialize_parameters(hidden_layers_dim,n_input,n_output,\"random\")\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Forward propagation <a class=\"anchor\" id=\"section_1_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_function(Z,activation_name):\n",
    "\n",
    "    if activation_name.lower() == \"sigmoid\":\n",
    "\n",
    "        A = 1/(1+np.exp(-Z))\n",
    "    \n",
    "    elif activation_name.lower() == \"relu\":\n",
    "\n",
    "        A = np.maximum(0,Z)\n",
    "\n",
    "    elif activation_name.lower() == \"tanh\":\n",
    "\n",
    "        A = np.tanh(Z)\n",
    "    \n",
    "    else:\n",
    "        # By default relu\n",
    "        A = np.maximum(0,Z)\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X,parameters,activation_name=\"relu\"):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute the activation function\n",
    "    \n",
    "    Arguments:\n",
    "    activation_name -- name of the activation function choosen\n",
    "    Z -- items\n",
    "\n",
    "    Returns:\n",
    "    activation -- activation value\n",
    "    \"\"\"\n",
    "    # init cache\n",
    "    caches = []\n",
    "    cache_layer = {}\n",
    "\n",
    "    # layer\n",
    "    L = len(parameters)//2\n",
    "\n",
    "    # setting A_prev to X\n",
    "    A_prev = X\n",
    "\n",
    "    for i in range(1,L+1):\n",
    "\n",
    "        # getting parameters\n",
    "        W = parameters[\"W\" + str(i)]\n",
    "        b= parameters[\"b\" + str(i)]\n",
    "\n",
    "        # linear result\n",
    "        Z = np.dot(W,A_prev) + b\n",
    "\n",
    "        if i==L:\n",
    "            # last layer -  sigmoid \n",
    "            A = activation_function(Z,\"sigmoid\")\n",
    "        else:\n",
    "            A = activation_function(Z,\"relu\")\n",
    "\n",
    "        # adding to the cache\n",
    "        cache = {\"W\" : W, \"b\":b,\"A\":A,\"Z\":Z,\"A_prev\": A_prev}\n",
    "\n",
    "        # adding layer cache\n",
    "        caches.append(cache)\n",
    "\n",
    "        # setting A_prev\n",
    "        A_prev = A\n",
    "\n",
    "    return A, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W shape: (5, 2) b shape: (5, 1)\n",
      "W shape: (5, 5) b shape: (5, 1)\n",
      "W shape: (3, 5) b shape: (3, 1)\n",
      "W shape: (1, 3) b shape: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "# test forward propagation\n",
    "\n",
    "layers_dim = [5,5,3]\n",
    "X = np.random.randn(2,100) *0.01\n",
    "params = initialize_parameters(layers_dim,X.shape[0],1)\n",
    "\n",
    "AL,caches = forward_propagation(X,params)\n",
    "\n",
    "# check the shape of W \n",
    "for val in caches:\n",
    "    print(\"W shape:\",val[\"W\"].shape,\"b shape:\",val[\"b\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Compute Loss <a class=\"anchor\" id=\"section_1_3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(AL,y):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute the log loss L(y_pred,y)\n",
    "\n",
    "    -- Arguments:\n",
    "    y : true labels of the dataset\n",
    "    AL : result of the forward propagation \n",
    "\n",
    "    -- Returns:\n",
    "    cost : Log loss cost\n",
    "\n",
    "    \"\"\"\n",
    "    # m examples\n",
    "    m = y.shape[1]\n",
    "\n",
    "    cost = -(1/m) *(np.dot(y,np.log(AL).T) + np.dot(1-y,np.log(1-AL).T))\n",
    "\n",
    "    return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost_function result: 0.9572727403146426\n",
      "log_loss sklearn: 0.9572727403146427\n"
     ]
    }
   ],
   "source": [
    "# Test the cost\n",
    "y_true = np.random.randint(0,2,(1,100))\n",
    "y_pred = np.random.random((1,100))\n",
    "\n",
    "# check with the true log_loss\n",
    "cost = cost_function(y_pred,y_true)\n",
    "l_cost = log_loss(y_true.T,y_pred.T)\n",
    "print(\"cost_function result:\",cost)\n",
    "print(\"log_loss sklearn:\",l_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Backward propagation <a class=\"anchor\" id=\"section_1_4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_activation(dA,Z,function_name=\"relu\"):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute dZ for the backward propagation\n",
    "\n",
    "    -- Arguments:\n",
    "    dA : derivative of A\n",
    "    Z : linear activation\n",
    "    function_name: name of the activation_function\n",
    "\n",
    "    -- Returns:\n",
    "    dZ: derivative of Z\n",
    "    \"\"\"\n",
    "\n",
    "    if function_name.lower() == \"sigmoid\":\n",
    "        # sigmoid\n",
    "        s = 1/(1+np.exp(-Z))\n",
    "\n",
    "        # derivative sigmoid\n",
    "        dG = s*(1-s)\n",
    "\n",
    "        # dZ\n",
    "        dZ = dA * dG\n",
    "\n",
    "    elif function_name.lower() == \"relu\":\n",
    "\n",
    "        # relu\n",
    "        r = np.maximum(0,Z)\n",
    "\n",
    "        # derivative relu\n",
    "        dG = np.int64(r>0)\n",
    "\n",
    "        # dZ\n",
    "        dZ = np.multiply(dA,dG)\n",
    "\n",
    "    elif function_name.lower() == \"tanh\":\n",
    "\n",
    "        # tanh\n",
    "        th = np.tanh(Z)\n",
    "\n",
    "        # derivative tanh\n",
    "        dG = 1-np.power(th,2)\n",
    "\n",
    "        # dZ\n",
    "        dZ = dA * dG\n",
    "\n",
    "    else:\n",
    "        # by default relu\n",
    "        r = np.maximum(0,Z)\n",
    "\n",
    "        # derivative relu\n",
    "        dG = np.int64(r>0)\n",
    "\n",
    "        # dZ\n",
    "        dZ = np.multiply(dA,dG)\n",
    "\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(AL,y,caches):\n",
    "\n",
    "    # gradients\n",
    "    gradients = {}\n",
    "\n",
    "    # numbers of layers\n",
    "    L = len(caches)\n",
    "\n",
    "    # number of examples\n",
    "    m = y.shape[1]\n",
    "\n",
    "    # dAL\n",
    "    dAL = - (np.divide(y, AL) - np.divide(1 - y, 1 - AL))\n",
    "    \n",
    "    # getting cache layer L\n",
    "    current_cache = caches[-1]\n",
    "    WL = current_cache['W']\n",
    "    ZL = current_cache['Z']\n",
    "    A_prev = current_cache['A_prev']\n",
    "\n",
    "    dZL = backward_activation(dAL,ZL,\"sigmoid\")\n",
    "\n",
    "    dW_temp = (1/m)*np.dot(dZL,A_prev.T)\n",
    "    db_temp = (1/m) * np.sum(dZL,axis=1, keepdims=True)\n",
    "    dA_prev_temp = np.dot(WL.T,dZL)\n",
    "    \n",
    "    # compute the gradient\n",
    "    gradients[\"dW\" + str(L)] = dW_temp\n",
    "    gradients[\"db\" + str(L)] = db_temp\n",
    "\n",
    "\n",
    "    for i in reversed(range(L-1)):\n",
    "\n",
    "        # getting cache layer L\n",
    "        current_cache = caches[i]\n",
    "        W = current_cache['W']\n",
    "        Z = current_cache['Z']\n",
    "        A_prev = current_cache['A_prev']\n",
    "\n",
    "        dZ = backward_activation(dA_prev_temp,Z,\"relu\")\n",
    "\n",
    "        dW_temp = (1/m)*np.dot(dZ,A_prev.T)\n",
    "        db_temp = (1/m) * np.sum(dZ,axis=1, keepdims=True)\n",
    "        dA_prev_temp = np.dot(W.T,dZ)\n",
    "        \n",
    "        # compute the gradient\n",
    "        gradients[\"dW\" + str(i+1)] = dW_temp\n",
    "        gradients[\"db\" + str(i+1)] = db_temp\n",
    "\n",
    "\n",
    "    return gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 (5, 10)\n",
      "dW2 (5, 5)\n",
      "dW3 (4, 5)\n",
      "dW4 (1, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-3e2a642804e2>:5: RuntimeWarning: overflow encountered in exp\n",
      "  A = 1/(1+np.exp(-Z))\n",
      "<ipython-input-30-a7161120fb19>:13: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  dAL = - (np.divide(y, AL) - np.divide(1 - y, 1 - AL))\n",
      "<ipython-input-30-a7161120fb19>:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "  dAL = - (np.divide(y, AL) - np.divide(1 - y, 1 - AL))\n",
      "<ipython-input-18-51b5cda1e506>:11: RuntimeWarning: invalid value encountered in multiply\n",
      "  dZ = dA * dG\n"
     ]
    }
   ],
   "source": [
    "# test backward propa\n",
    "\n",
    "layers_dim = [5,5,4]\n",
    "X = np.random.randn(10,100)\n",
    "y_true = np.random.randint(0,2,(1,100))\n",
    "params = initialize_parameters(layers_dim,X.shape[0],1)\n",
    "\n",
    "AL,caches = forward_propagation(X,params)\n",
    "    \n",
    "gradients = backward_propagation(AL,y_true,caches)\n",
    "gradients\n",
    "for i in range(len(caches)):\n",
    "    print(f\"dW{i+1}\",gradients[f\"dW{i+1}\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Update parameters <a class=\"anchor\" id=\"section_1_5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters,gradients,learning_rate):\n",
    "\n",
    "    # copy \n",
    "    params = copy.deepcopy(parameters)\n",
    "    \n",
    "    # nb layer\n",
    "    L = len(parameters)//2\n",
    "\n",
    "    for i in range(L):\n",
    "        params[\"W\" + str(i+1)] = params[\"W\" + str(i+1)] - learning_rate * gradients[\"dW\" + str(i+1)]\n",
    "        params[\"b\" + str(i+1)] = params[\"b\" + str(i+1)] - learning_rate * gradients[\"db\" + str(i+1)]\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Predict <a class=\"anchor\" id=\"section_1_6\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,parameters,function_name):\n",
    "\n",
    "    y_pred,caches = forward_propagation(X,parameters,function_name)\n",
    "\n",
    "    y_pred = np.where(y_pred>=0.5,1,0)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(X,y,parameters):\n",
    "    \n",
    "    # prediction\n",
    "    y_pred = predict(X,parameters,function_name=\"relu\")\n",
    "\n",
    "    # score\n",
    "    acc = accuracy_score(y.T,y_pred.T)\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Model <a class=\"anchor\" id=\"section_1_7\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_model(X_train,y_train,X_test,y_test,hidden_layers_dims,n_iter,learning_rate,function_name=\"relu\",initialization_weights=\"random\"):\n",
    "\n",
    "    # init trackers\n",
    "    dico_model = {}\n",
    "    costs_train = []\n",
    "    costs_test = []\n",
    "    accs_train = []\n",
    "    accs_test = []\n",
    "\n",
    "    # init\n",
    "    m = X.shape[1]\n",
    "    n_input = X_train.shape[0]\n",
    "    n_output = y_train.shape[0]\n",
    "\n",
    "    # initialize parameters\n",
    "    parameters = initialize_parameters(hidden_layers_dims,n_input,n_output,initialization_weights)\n",
    "\n",
    "\n",
    "    for i in range(n_iter):\n",
    "\n",
    "        # ---- forward propagation ------\n",
    "        AL_train, caches = forward_propagation(X_train,parameters, activation_name=function_name)\n",
    "        AL_test, caches_test = forward_propagation(X_test,parameters, activation_name=function_name)\n",
    "\n",
    "        #  cost\n",
    "        cost_train = cost_function(AL_train,y_train)\n",
    "        costs_train.append(cost_train)\n",
    "\n",
    "        cost_test = cost_function(AL_test,y_test)\n",
    "        costs_test.append(cost_test)\n",
    "\n",
    "        # accuracy \n",
    "        y_pred_train = predict(X_train,parameters,function_name)\n",
    "        acc_train = accuracy_score(y_train.T,y_pred_train.T)\n",
    "        accs_train.append(acc_train)\n",
    "\n",
    "        y_pred_test = predict(X_test,parameters,function_name)\n",
    "        acc_test = accuracy_score(y_test.T,y_pred_test.T)\n",
    "        accs_test.append(acc_test)    \n",
    "\n",
    "        # ---- bacward propagation ------\n",
    "        gradients = backward_propagation(AL_train,y_train,caches,function_name)\n",
    "\n",
    "        # ---- update params ------------\n",
    "        params = update_parameters(parameters, gradients, learning_rate)\n",
    "        parameters = params\n",
    "\n",
    "\n",
    "    dict_model = {\"parameters\": parameters,\"cost_train\":costs_train,\"accuracy_train\":accs_train,\"cost_test\":costs_test,\"accuracy_test\":accs_test}\n",
    "\n",
    "    return dict_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Example on dataset 1 <a class=\"anchor\" id=\"chapter2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Example on dataset 2 <a class=\"anchor\" id=\"chapter3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d46af94c2bbce495f1e668725902fa517c90b1782bcfe2fce0dd9868df553d3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
