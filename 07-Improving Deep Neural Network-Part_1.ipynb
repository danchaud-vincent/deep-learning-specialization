{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Deep Neural Network - Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [1. Setting up your ML application](#chapter1)\n",
    "    * [1.1  Train/dev/test](#section_1_1)\n",
    "    * [1.2  Bias vs Variance](#section_1_2)\n",
    "    * [1.3  Basic recipe of ML](#section_1_3)\n",
    "* [2. Regularizing your Neural Network](#chapter2)\n",
    "    * [2.1  Regularization](#section_2_1)\n",
    "    * [2.2  Why Regularization reduces overfitting](#section_2_2)\n",
    "    * [2.3  Dropout Regularization](#section_2_3)\n",
    "    * [2.4  Other Regularization](#section_2_4)\n",
    "* [3. Setting up your Optimization problem](#chapter3)\n",
    "    * [3.1 ](#section_3_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know how to implement a neural network. Now we will learn how to make our neural network work well.\n",
    "-  hyperparameter tuning \n",
    "-  set up your data\n",
    "-  make sure your optimization algorithm runs quickly so that you get your learning algorithm to learn in a reasonable time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setting up your ML application <a class=\"anchor\" id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train our neural network we have to make a lot of decisions such as :\n",
    "- number of layers in the neural network\n",
    "- number of hidden units\n",
    "- learning rate\n",
    "- activation function\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the good parameters we have to test different models.\n",
    "\n",
    "So in pratice applied Machine Learning is a highly iterative process\n",
    "\n",
    "<center><img src=\"images/07-Improving Deep Neural Network/process-ML.PNG\" width = \"3a00px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You start with an idea\n",
    "- you code it up and try it by running your code\n",
    "- you experiment and you get back a result that tells you how well work your network\n",
    "\n",
    "And then you refine your idea and try a new one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Train/Dev/Test <a class=\"anchor\" id=\"section_1_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up your dataset in terms of your train, development and test sets can make you much more efficient at determine more quickly a good model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/07-Improving Deep Neural Network/dataset.PNG\" width = \"400px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You train your algorithms on the <b>Training set </b>\n",
    "- you use your <b>Dev set</b> to see which of many different models performs best \n",
    "- you evaluate the best model on your <b>Testing set</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the previous era, or on small dataset :\n",
    "\n",
    "We split the dataset on :\n",
    "- 70% / 30%\n",
    "- 60% / 20% / 20%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Big data (Big dataset):\n",
    "\n",
    "\n",
    "We split the dataset on :\n",
    "- 98% / 1% / 1%\n",
    "- 99.5% / 0.5% / 0.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mismatched Train/Test distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex :\n",
    "- Training set: Cat pictures from web\n",
    "- Dev/ test sets: Cat pictures from users using your app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Make sure the dev and test set come from the same distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So having set up a train dev and test set will allow you to integrate more quickly. It will also allow you to more efficiently measure the bias and variance of your algorithm so you can more efficiently select ways to improve your algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Bias vs Variance <a class=\"anchor\" id=\"section_1_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/07-Improving Deep Neural Network/bias-variance.PNG\" width = \"500px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Deep learning the two key numbers to look at to understand bias and variance will be the train set error and the dev set or the development set error.\n",
    "\n",
    "> For example we take a Cat Classifier:\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>Train set Error</td>\n",
    "            <td>Dev set Error</td>\n",
    "            <td>Bias-Variance</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>1%</td>\n",
    "            <td>11%</td>\n",
    "            <td>High variance</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>15%</td>\n",
    "            <td>16%</td>\n",
    "            <td>High Biais</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>15%</td>\n",
    "            <td>30%</td>\n",
    "            <td>High variance & High bias</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>0.5%</td>\n",
    "            <td>1%</td>\n",
    "            <td>Low bias & Low variance</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Basic recipe of ML <a class=\"anchor\" id=\"section_1_3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- High Bias (Training set performance):\n",
    "    - Test bigger Network\n",
    "    - Train longer\n",
    "\n",
    "- High Variance (Dev set performance):\n",
    "    - Regularization\n",
    "    - More Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Regularizing your Neural Network <a class=\"anchor\" id=\"chapter2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Regularization <a class=\"anchor\" id=\"section_2_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Logistic Regression :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(W,b) = \\frac{1}{m} \\sum_{i=1}^{m} L(y_{pred},y) + \\frac{\\lambda}{2m} \\lVert w \\rVert^2_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L2 Regularization :\n",
    "\n",
    " $$\\lVert w \\rVert^2_2  = \\sum_{j=1}^{n_x}w_j^2 = W^TW  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L1 Regularization :\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\lambda}{2m} \\sum_{j=1}^{n_x} |w_j| = \\frac{\\lambda}{2m} \\lVert w \\rVert_1     $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Neural Network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(W^{[1]},b^{[1]},W^{[2]},b^{[2]}....,W^{[L]},b^{[L]}) = \\frac{1}{m} \\sum_{i=1}^{m} L(y_{pred},y) + \\frac{\\lambda}{2m} \\sum_{l=1}^{L} \\lVert W^{[l]} \\rVert^2_F$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization:\n",
    "\n",
    "$$\\lVert W^{[l]} \\rVert^2_F = \\sum_{i=1}^{n^{[l]}}  \\sum_{j=1}^{n^{[l-1]}} (w_{ij}^{[l]})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Backpropagation :\n",
    "\n",
    "$$ dW^{[l]} = (From \\ \\ backprop) + \\frac{\\lambda}{m} W^{[l]} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Why Regularization reduces overfitting? <a class=\"anchor\" id=\"section_2_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Example :\n",
    "\n",
    "Let's take for example a neural network with overfitting, so High Variance.\n",
    "\n",
    "We add the extra term <b>regularization</b> in the loss function:\n",
    "\n",
    "$$J= \\frac{1}{m} \\sum_{i=1}^{m} L(y_{pred},y) + \\frac{\\lambda}{2m} \\sum_{l=1}^{L} \\lVert W^{[l]} \\rVert^2_F$$\n",
    "\n",
    "<b>It penalizes the weight matrices from being too large.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If for example we set lambda to be really big\n",
    "- Then it set the weight matrices W to be reasonably close to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/07-Improving Deep Neural Network/nn_example.PNG\" width = \"300px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/07-Improving Deep Neural Network/nn_simplified.PNG\" width = \"300px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Some hidden units will be close to zero. And therefor the neural network will be simplified and become smaller.So that will take you from this overfitting case much closer to the high bias case.</b> By testing different value of lambda you will reduce the overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tanh Example :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/07-Improving Deep Neural Network/tanh_function.png\" width = \"300px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If Lambda is big, then W becomes smaller. So Z[l] = W[l]. A[l-1] + b[l] will be very small. If Z is small, it can take values close to 0. And then <b>tanh = g(Z) will be roughly linear</b>. The activation function will be relatively linear. And we saw in a previous course that if a layer is linear then the all neural network is linear. So By computing a relativaly linear activation function, we will have a neural network relatively linear than a high-complex non linear function. So it will reduce the complexity of our model and therefor reduce the overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Dropout Regularization <a class=\"anchor\" id=\"section_2_3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Other Regularization <a class=\"anchor\" id=\"section_2_4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> DATA AUGMENTATION :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting more training data can be expensive and sometimes you just can't get more data. A free way to have more data is to apply <b>Data Augmentation</b> on your images.\n",
    "\n",
    "- By flipping the images horizontally, you could double the size of your training set.\n",
    "- Randomly Zoom to the image\n",
    "\n",
    "<center><img src=\"images/07-Improving Deep Neural Network/data-augmentation.PNG\" width = \"600px\"></center>\n",
    "\n",
    "So by taking random distortions and translations of the image you could augment your data set and make additional fake training examples. This can be an inexpensive way to give your algorithm more data and therefore sort of regularize it and reduce over fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Early stopping you plot the Cost function J that you try to optimize. And you plot also your Dev set Error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/07-Improving Deep Neural Network/early-stopping.PNG\" width = \"400px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term early stopping refers to the fact that you're just stopping the training of your neural network earlier.\n",
    "\n",
    "And as you iterate, as you train, w will get bigger and bigger and bigger until here maybe you have a much larger value of the parameters w for your neural network. So what early stopping does is by stopping halfway you have only a mid-size rate w. And so similar to L2 regularization by picking a neural network with smaller norm for your parameters w, hopefully your neural network is over fitting less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Machine learning project you want an algorithm :\n",
    "- to optimize the cost function j (tools: gradient descent ..)\n",
    "-  after optimizing the cost function j, you also wanted to not over-fit. (regularization...)\n",
    "And to realize these tasks you use different tools.\n",
    "\n",
    "The main downside of early stopping is that this couples these two tasks, because by stopping gradient decent early, you're sort of breaking whatever you're doing to optimize cost function J, because now you're not doing a great job reducing the cost function J. And then you also simultaneously trying to not over fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Setting up your optimization problem <a class=\"anchor\" id=\"chapter3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing Inputs | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Normalizing Inputs <a class=\"anchor\" id=\"section_3_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing your inputs corresponds to two steps:\n",
    "- the first is to subtract out or to zero out the mean\n",
    "- then the second step is to normalize the variances\n",
    "\n",
    "$$ X_{standard} = \\frac{X-\\mu}{\\sigma}$$\n",
    "\n",
    "<u>Example</u> :\n",
    "<center><img src=\"images/07-Improving Deep Neural Network/normalization-steps.PNG\" width = \"500px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your input features came from very different scales, maybe some features are from 0-1, sum from 1-1000, then it's important to normalize your features.\n",
    "\n",
    "- That just makes your cost function j easier and faster to optimize.\n",
    "- If the data are not on the same scales. That really hurts your optimization algorithm. The gradient decent might need a lot of steps to oscillate back and forth before it finally finds its way to the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>2D Example</u>:\n",
    "\n",
    "<center><img src=\"images/07-Improving Deep Neural Network/normalization-ex.PNG\" width = \"500px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>IMPORTANT :</b> User the same mu and sigma to normalize the testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2  Exploding Gradients and Weights Initialization <a class=\"anchor\" id=\"section_3_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
