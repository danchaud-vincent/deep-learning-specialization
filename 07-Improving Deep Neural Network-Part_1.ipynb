{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Deep Neural Network - Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [1. Setting up your ML application](#chapter1)\n",
    "    * [1.1  Train/dev/test](#section_1_1)\n",
    "    * [1.2  Bias vs Variance](#section_1_2)\n",
    "    * [1.3  Basic recipe of ML](#section_1_3)\n",
    "* [2. Regularizing your Neural Network](#chapter2)\n",
    "    * [2.1  Regularization](#section_2_1)\n",
    "    * [2.2  Why Regularization reduces overfitting](#section_2_2)\n",
    "    * [2.3  Dropout Regularization](#section_2_3)\n",
    "    * [2.4  Other Regularization](#section_2_4)\n",
    "* [3. Setting up your Optimization problem](#chapter3)\n",
    "    * [3.1 Normalizing Inputs](#section_3_1)\n",
    "    * [3.2 Exploding Gradients and Weights Initialization](#section_3_2)\n",
    "    * [3.3 gradient checking](#section_3_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know how to implement a neural network. Now we will learn how to make our neural network work well.\n",
    "-  hyperparameter tuning \n",
    "-  set up your data\n",
    "-  make sure your optimization algorithm runs quickly so that you get your learning algorithm to learn in a reasonable time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setting up your ML application <a class=\"anchor\" id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train our neural network we have to make a lot of decisions such as :\n",
    "- number of layers in the neural network\n",
    "- number of hidden units\n",
    "- learning rate\n",
    "- activation function\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the good parameters we have to test different models.\n",
    "\n",
    "So in pratice applied Machine Learning is a highly iterative process\n",
    "\n",
    "<center><img src=\"images/07-Improving Deep Neural Network/process-ML.PNG\" width = \"3a00px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You start with an idea\n",
    "- you code it up and try it by running your code\n",
    "- you experiment and you get back a result that tells you how well work your network\n",
    "\n",
    "And then you refine your idea and try a new one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Train/Dev/Test <a class=\"anchor\" id=\"section_1_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up your dataset in terms of your train, development and test sets can make you much more efficient at determine more quickly a good model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/07-Improving Deep Neural Network/dataset.PNG\" width = \"400px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You train your algorithms on the <b>Training set </b>\n",
    "- you use your <b>Dev set</b> to see which of many different models performs best \n",
    "- you evaluate the best model on your <b>Testing set</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the previous era, or on small dataset :\n",
    "\n",
    "We split the dataset on :\n",
    "- 70% / 30%\n",
    "- 60% / 20% / 20%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Big data (Big dataset):\n",
    "\n",
    "\n",
    "We split the dataset on :\n",
    "- 98% / 1% / 1%\n",
    "- 99.5% / 0.5% / 0.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mismatched Train/Test distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex :\n",
    "- Training set: Cat pictures from web\n",
    "- Dev/ test sets: Cat pictures from users using your app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Make sure the dev and test set come from the same distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So having set up a train dev and test set will allow you to integrate more quickly. It will also allow you to more efficiently measure the bias and variance of your algorithm so you can more efficiently select ways to improve your algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Bias vs Variance <a class=\"anchor\" id=\"section_1_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/07-Improving Deep Neural Network/bias-variance.PNG\" width = \"500px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Deep learning the two key numbers to look at to understand bias and variance will be the train set error and the dev set or the development set error.\n",
    "\n",
    "> For example we take a Cat Classifier:\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>Train set Error</td>\n",
    "            <td>Dev set Error</td>\n",
    "            <td>Bias-Variance</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>1%</td>\n",
    "            <td>11%</td>\n",
    "            <td>High variance</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>15%</td>\n",
    "            <td>16%</td>\n",
    "            <td>High Biais</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>15%</td>\n",
    "            <td>30%</td>\n",
    "            <td>High variance & High bias</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>0.5%</td>\n",
    "            <td>1%</td>\n",
    "            <td>Low bias & Low variance</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Basic recipe of ML <a class=\"anchor\" id=\"section_1_3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- High Bias (Training set performance):\n",
    "    - Test bigger Network\n",
    "    - Train longer\n",
    "\n",
    "- High Variance (Dev set performance):\n",
    "    - Regularization\n",
    "    - More Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Regularizing your Neural Network <a class=\"anchor\" id=\"chapter2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Regularization <a class=\"anchor\" id=\"section_2_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Logistic Regression :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(W,b) = \\frac{1}{m} \\sum_{i=1}^{m} L(y_{pred},y) + \\frac{\\lambda}{2m} \\lVert w \\rVert^2_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L2 Regularization :\n",
    "\n",
    " $$\\lVert w \\rVert^2_2  = \\sum_{j=1}^{n_x}w_j^2 = W^TW  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L1 Regularization :\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\lambda}{2m} \\sum_{j=1}^{n_x} |w_j| = \\frac{\\lambda}{2m} \\lVert w \\rVert_1     $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Neural Network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(W^{[1]},b^{[1]},W^{[2]},b^{[2]}....,W^{[L]},b^{[L]}) = \\frac{1}{m} \\sum_{i=1}^{m} L(y_{pred},y) + \\frac{\\lambda}{2m} \\sum_{l=1}^{L} \\lVert W^{[l]} \\rVert^2_F$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization:\n",
    "\n",
    "$$\\lVert W^{[l]} \\rVert^2_F = \\sum_{i=1}^{n^{[l]}}  \\sum_{j=1}^{n^{[l-1]}} (w_{ij}^{[l]})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Backpropagation :\n",
    "\n",
    "$$ dW^{[l]} = (From \\ \\ backprop) + \\frac{\\lambda}{m} W^{[l]} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Why Regularization reduces overfitting? <a class=\"anchor\" id=\"section_2_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Example :\n",
    "\n",
    "Let's take for example a neural network with overfitting, so High Variance.\n",
    "\n",
    "We add the extra term <b>regularization</b> in the loss function:\n",
    "\n",
    "$$J= \\frac{1}{m} \\sum_{i=1}^{m} L(y_{pred},y) + \\frac{\\lambda}{2m} \\sum_{l=1}^{L} \\lVert W^{[l]} \\rVert^2_F$$\n",
    "\n",
    "<b>It penalizes the weight matrices from being too large.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If for example we set lambda to be really big\n",
    "- Then it set the weight matrices W to be reasonably close to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/07-Improving Deep Neural Network/nn_example.PNG\" width = \"300px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/07-Improving Deep Neural Network/nn_simplified.PNG\" width = \"300px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Some hidden units will be close to zero. And therefor the neural network will be simplified and become smaller.So that will take you from this overfitting case much closer to the high bias case.</b> By testing different value of lambda you will reduce the overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tanh Example :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/07-Improving Deep Neural Network/tanh_function.png\" width = \"300px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If Lambda is big, then W becomes smaller. So Z[l] = W[l]. A[l-1] + b[l] will be very small. If Z is small, it can take values close to 0. And then <b>tanh = g(Z) will be roughly linear</b>. The activation function will be relatively linear. And we saw in a previous course that if a layer is linear then the all neural network is linear. So By computing a relativaly linear activation function, we will have a neural network relatively linear than a high-complex non linear function. So it will reduce the complexity of our model and therefor reduce the overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is L2-regularization actually doing?**:\n",
    "\n",
    "L2-regularization relies on the assumption that a model with small weights is simpler than a model with large weights. Thus, by penalizing the square values of the weights in the cost function you drive all the weights to smaller values. It becomes too costly for the cost to have large weights! This leads to a smoother model in which the output changes more slowly as the input changes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Dropout Regularization <a class=\"anchor\" id=\"section_2_3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, **dropout** is a widely used regularization technique that is specific to deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/07-Improving Deep Neural Network/dropout1.PNG\" width = \"300px\" height=\"160px\"><img src=\"images/07-Improving Deep Neural Network/dropout2.PNG\" width = \"300px\" height=\"160px\"><img src=\"images/07-Improving Deep Neural Network/dropout3.PNG\" width = \"300px\" height=\"160px\"><img src=\"images/07-Improving Deep Neural Network/dropout4.PNG\" width = \"300px\" height=\"160px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>Figure</b>:<b>Drop-out on the second hidden layer.</b> <br> At each iteration, you shut down (= set to zero) each neuron of a layer with probability 1 - keep_prob or keep it with probability keep_prob (50% here). The dropped neurons don't contribute to the training in both the forward and backward propagations of the iteration.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/07-Improving Deep Neural Network/dropout-it1.PNG\" width = \"37%\"><img src=\"images/07-Improving Deep Neural Network/dropout-it2.PNG\" width = \"38%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>Figure </b>:<b> Drop-out on the first and third hidden layers. </b><br> 1st layer: we shut down on average 40% of the neurons. 3rd layer: we shut down on average 20% of the neurons.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you shut some neurons down, you actually modify your model. The idea behind drop-out is that at each iteration, you train a different model that uses only a subset of your neurons. With dropout, your neurons thus become less sensitive to the activation of one other specific neuron, because that other neuron might be shut down at any time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Other Regularization <a class=\"anchor\" id=\"section_2_4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> DATA AUGMENTATION :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting more training data can be expensive and sometimes you just can't get more data. A free way to have more data is to apply <b>Data Augmentation</b> on your images.\n",
    "\n",
    "- By flipping the images horizontally, you could double the size of your training set.\n",
    "- Randomly Zoom to the image\n",
    "\n",
    "<center><img src=\"images/07-Improving Deep Neural Network/data-augmentation.PNG\" width = \"600px\"></center>\n",
    "\n",
    "So by taking random distortions and translations of the image you could augment your data set and make additional fake training examples. This can be an inexpensive way to give your algorithm more data and therefore sort of regularize it and reduce over fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Early stopping you plot the Cost function J that you try to optimize. And you plot also your Dev set Error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/07-Improving Deep Neural Network/early-stopping.PNG\" width = \"400px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term early stopping refers to the fact that you're just stopping the training of your neural network earlier.\n",
    "\n",
    "And as you iterate, as you train, w will get bigger and bigger and bigger until here maybe you have a much larger value of the parameters w for your neural network. So what early stopping does is by stopping halfway you have only a mid-size rate w. And so similar to L2 regularization by picking a neural network with smaller norm for your parameters w, hopefully your neural network is over fitting less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Machine learning project you want an algorithm :\n",
    "- to optimize the cost function j (tools: gradient descent ..)\n",
    "-  after optimizing the cost function j, you also wanted to not over-fit. (regularization...)\n",
    "And to realize these tasks you use different tools.\n",
    "\n",
    "The main downside of early stopping is that this couples these two tasks, because by stopping gradient decent early, you're sort of breaking whatever you're doing to optimize cost function J, because now you're not doing a great job reducing the cost function J. And then you also simultaneously trying to not over fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Setting up your optimization problem <a class=\"anchor\" id=\"chapter3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Normalizing Inputs <a class=\"anchor\" id=\"section_3_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing your inputs corresponds to two steps:\n",
    "- the first is to subtract out or to zero out the mean\n",
    "- then the second step is to normalize the variances\n",
    "\n",
    "$$ X_{standard} = \\frac{X-\\mu}{\\sigma}$$\n",
    "\n",
    "<u>Example</u> :\n",
    "<center><img src=\"images/07-Improving Deep Neural Network/normalization-steps.PNG\" width = \"500px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your input features came from very different scales, maybe some features are from 0-1, sum from 1-1000, then it's important to normalize your features.\n",
    "\n",
    "- That just makes your cost function j easier and faster to optimize.\n",
    "- If the data are not on the same scales. That really hurts your optimization algorithm. The gradient decent might need a lot of steps to oscillate back and forth before it finally finds its way to the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>2D Example</u>:\n",
    "\n",
    "<center><img src=\"images/07-Improving Deep Neural Network/normalization-ex.PNG\" width = \"500px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>IMPORTANT :</b> User the same mu and sigma to normalize the testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2  Exploding Gradients and Weights Initialization <a class=\"anchor\" id=\"section_3_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the problems of training neural network, especially very deep neural networks, is data vanishing and exploding gradients. What that means is that when you're training a very deep network your derivatives or your slopes can sometimes get either very, very big or very, very small, maybe even exponentially small, and this makes training difficult.\n",
    "\n",
    "\n",
    "In general, initializing all the weights to zero results in the network failing to break symmetry. This means that every neuron in each layer will learn the same thing, so you might as well be training a neural network with $n^{[l]}=1$ for every layer. This way, the network is no more powerful than a linear classifier like logistic regression. \n",
    "\n",
    "It turns out there's a partial solution that doesn't completely solve this problem but it helps a lot which is careful choice of how you initialize the weights.\n",
    "\n",
    "A well-chosen initialization method helps the learning process. A well-chosen initialization can:\n",
    "- Speed up the convergence of gradient descent\n",
    "- Increase the odds of gradient descent converging to a lower training (and generalization) error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Initialization Weights:\n",
    "\n",
    "- Relu : \n",
    "$$ np.random.randn(shape) * np.sqrt(\\frac{2}{n^{[l-1]}})$$\n",
    "\n",
    "- tanh :\n",
    "$$ np.random.randn(shape) * np.sqrt(\\frac{1}{n^{[l-1]}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3  Gradient Checking <a class=\"anchor\" id=\"section_3_3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Gradient Checking steps:\n",
    "\n",
    "- Take W[1], b[1],W[2], b[2],....,W[L], b[L] and reshape into a big vector Theta\n",
    "- Take dW[1], db[1], dW[2], db[2],...., dW[L], db[L] and reshape into a big vector dTheta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ J(W^{[1]},b^{[1]},.....,W^{[L]},b^{[L]}) = J(\\theta^1,...,\\theta^L)$$\n",
    "\n",
    "For each i :\n",
    "$$ d\\theta_{approx}^{[i]} = \\frac{J(\\theta^1,.,\\theta^i + \\epsilon.,\\theta^L)-J(\\theta^1,.,\\theta^i - \\epsilon.,\\theta^L)}{2\\epsilon}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Check:\n",
    "\n",
    "$$ esp = \\frac{ \\lVert d\\theta_{approx} - d\\theta \\rVert_2}{\\lVert d\\theta_{approx} \\rVert_2 + \\lVert d\\theta \\rVert_2} $$\n",
    "\n",
    "- esp ~= 10^-7 | great!\n",
    "- esp ~= 10^-5 | ok!\n",
    "- esp ~= 10^-3 | worry!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Gradient checking implementation notes:\n",
    "\n",
    "- Don't use in training - only to debug\n",
    "- if the algorithm fails grad check, look at components to try to identify bug\n",
    "- Remember regularization\n",
    "- Doesn't work with Dropout\n",
    "- Run at random initialization"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
